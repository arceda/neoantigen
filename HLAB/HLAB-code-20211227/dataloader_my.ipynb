{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLAB dataloader adapted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataloader es una version actualizada de HLAB adaptado a la BD utilzada por BERTMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from bin.model_utils import BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Load_Dataset(Dataset):\n",
    "    def __init__(self, path, tokenizer_name='../../models/prot_bert_bfd', max_length=51):                  \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "\n",
    "        self.seqs, self.labels = self.load_dataset(path)        \n",
    "        self.max_length = max_length\n",
    "\n",
    "    # usadao por HLAB, nosotros ya tenemos las seudosecuencias\n",
    "    \"\"\"def HLA_trans(self):\n",
    "        HLA_seq = pd.read_csv('source/MHC_pseudo.dat', sep='\\t')\n",
    "        seqs = {}\n",
    "        for i in range(len(HLA_seq)):\n",
    "            seqs[HLA_seq.HLA[i]] = HLA_seq.sequence[i]\n",
    "        return seqs\n",
    "    \"\"\"\n",
    "    def transform(self, HLA, peptide):\n",
    "        data = HLA + peptide\n",
    "        data = data + 'X' * (49 - len(data)) # no usa el max length\n",
    "        return data\n",
    "\n",
    "    def read_and_prepare(self,file):\n",
    "        data = pd.read_csv(file)\n",
    "        \"\"\" # de HLAB original\n",
    "        seqs = self.HLA_trans()\n",
    "        data['cost_cents'] = data.apply(\n",
    "            lambda row: self.transform(\n",
    "                HLA=seqs[row['HLA'][0:5]+row['HLA'][6:]],\n",
    "                peptide=row['peptide']),\n",
    "            axis=1)\n",
    "        return np.vstack(data.cost_cents)\"\"\"\n",
    "        data['cost_cents'] = data.apply(\n",
    "            lambda row: self.transform(HLA=row['mhc'], peptide=row['peptide']), axis=1)\n",
    "        return np.vstack(data.cost_cents)\n",
    "\n",
    "    def get_label(self,file):\n",
    "        data = pd.read_csv(file)\n",
    "        label = []\n",
    "        #label.append(data['Label'].values)\n",
    "        label.append(data['masslabel'].values) # netMHCpan3.2 database\n",
    "        return label\n",
    "\n",
    "    def load_dataset(self,data_path):\n",
    "        file = data_path\n",
    "        df = pd.read_csv(file)\n",
    "        y_label = self.get_label(file)[0]\n",
    "        X_test = self.read_and_prepare(file)\n",
    "        X_test = X_test.tolist()\n",
    "        X_test = [' '.join(eachseq) for eachseq in X_test]\n",
    "        X_test = [\" \".join(eachseq) for eachseq in\n",
    "                  X_test]  # ['Y D S E Y R N I F T N T D E S N L Y L S Y N Y Y T W A V D A Y T W Y H M M V I F R L M',.....,'Y D S E Y R N I F T N T D E S N L Y L S Y N Y Y T W A V D A Y T W Y N F L I K F L L I']\n",
    "\n",
    "        return (X_test, y_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq = re.sub(r\"[UZOBJ]\", \"X\", seq).upper()\n",
    "\n",
    "        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "\n",
    "        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../../models/prot_bert_bfd\"\n",
    "train_dataset = My_Load_Dataset(path=\"../../dataset/netMHCIIpan3.2/train_mini.csv\", tokenizer_name=model_name, max_length=51)\n",
    "val_dataset = My_Load_Dataset(path=\"../../dataset/netMHCIIpan3.2/eval_mini.csv\", tokenizer_name=model_name, max_length=51)\n",
    "test_dataset = My_Load_Dataset(path=\"../../dataset/netMHCIIpan3.2/test_mini.csv\", tokenizer_name=model_name, max_length=51)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 2, 18,  9, 19, 19, 11,  6, 10,  7,  6,  6,  8, 14,  6, 11, 21,  9, 10,\n",
      "        10, 19, 14, 20, 19, 14, 11, 14,  9,  6, 15, 20, 22,  8,  8, 19, 15, 15,\n",
      "        11, 16,  5,  8,  6,  5, 15,  5, 15, 10, 20,  5,  7,  5,  3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1]), 'labels': tensor(1)}\n",
      "{'input_ids': tensor([ 2, 18,  9, 19, 19, 11,  6, 10,  7,  6,  6,  8, 14,  6, 11, 21,  9,  5,\n",
      "        10, 19,  9, 20, 20,  8,  5, 18, 12, 18, 17, 20, 22,  8,  8, 19, 15,  5,\n",
      "        10,  9,  9, 12,  8, 16, 24, 14, 18,  8,  8, 21, 15, 10,  3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_dataset[1])\n",
    "#print(train_dataset[0]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    prediction=pred.predictions\n",
    "    preds = prediction.argmax(-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    sn = tp / (tp + fn)\n",
    "    sp = tn / (tn + fp)\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'sn': sn,\n",
    "        'sp': sp,\n",
    "        'accuracy': acc,\n",
    "        'mcc': mcc\n",
    "    }\n",
    "\n",
    "config = BertConfig.from_pretrained(\"../../models/prot_bert_bfd\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"results/\",  # output directory\n",
    "        num_train_epochs=10,  # total number of training epochs\n",
    "        per_device_train_batch_size = 32,  # batch size per device during training\n",
    "        per_device_eval_batch_size = 32,  # batch size for evaluation\n",
    "        warmup_steps = 1000,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay = 0.01,  # strength of weight decay\n",
    "        learning_rate = 5e-5,  # The initial learning rate for optimizer.\n",
    "        logging_dir=\"results/\",  # directory for storing logs './logs'\n",
    "        logging_steps=1052,  # How often to print logs\n",
    "        save_steps=1052,        \n",
    "        eval_steps=1052,  # How often to eval\n",
    "        gradient_accumulation_steps=16,  # total number of steps before back propagation       \n",
    "    )\n",
    "\n",
    "model = Trainer(        \n",
    "        args=training_args,  # training arguments, defined above\n",
    "        model=BertForSequenceClassification.from_pretrained(model_name, config=config),  # ProBERT\n",
    "        # model=ProteinBertSequenceClsRnn.from_pretrained(model_name, config=config),       # ProBERT+BiLSTM\n",
    "        # model=ProteinBertSequenceClsRnnAtt.from_pretrained(model_name, config=config),    # ProBERT+BiLSTM+Attention\n",
    "        # model=ProteinBertSequenceClsCnn.from_pretrained(model_name, config=config),       # ProBERT+CNN\n",
    "        train_dataset=train_dataset,  # training dataset\n",
    "        eval_dataset=val_dataset,  # evaluation dataset\n",
    "        compute_metrics=compute_metrics,  # evaluation metrics\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
