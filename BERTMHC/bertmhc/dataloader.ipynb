{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este script, se detalla el dataloader para preparar el training set. EL training set de ejemplo esta en el archivo train_old.csv\n",
    "De dicho archivo, se utiliza la columna 'mhc' y 'peptide' concatenadas como input y el target esta compuesto por las columnas 'label' y 'masslabel'. Tambien se utiliza un la función collate_fn de pytorch para asegurar el mismo tamaño de los inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection, Mapping\n",
    "from pathlib import Path\n",
    "from tape.tokenizers import TAPETokenizer\n",
    "from tape.datasets import pad_sequences as tape_pad\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bertmhc import BERTMHC, BERTMHC_CNN\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils_model import EarlyStopping\n",
    "from utils_model import train, evaluate\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_file: Union[str, Path, pd.DataFrame],\n",
    "                 max_pep_len=60,\n",
    "                 train: bool = True):\n",
    "        if isinstance(data_file, pd.DataFrame):\n",
    "            data = data_file\n",
    "        else:\n",
    "            data = pd.read_csv(data_file)\n",
    "        mhc = data['mhc']\n",
    "        self.mhc = mhc.values\n",
    "        peptide = data['peptide']\n",
    "        peptide = peptide.apply(lambda x: x[:max_pep_len])\n",
    "        self.peptide = peptide.values\n",
    "        if not train:\n",
    "            data['label'] = np.nan\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'masslabel' not in data and 'label' not in data:\n",
    "            raise ValueError(\"missing label.\")\n",
    "        if 'masslabel' not in data:\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'label' not in data:\n",
    "            data['label'] = np.nan\n",
    "\n",
    "        ###########################################################################################################\n",
    "        ##### el target esta compuesto por el label(float) y masslabel(int) #######################################\n",
    "        self.targets = np.stack([data['label'], data['masslabel']], axis=1)\n",
    "        self.data = data        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mhc)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        ###########################################################################################################\n",
    "        ##### aqui concatena el MHC con el peptido para que todo eso sea el input #################################\n",
    "        seq = self.mhc[index] + self.peptide[index]\n",
    "        \n",
    "        # aqui hacemos padding y reemplazamos algunos aminoacidos\n",
    "        seq = seq + 'X' * (58 - len(seq)) \n",
    "        seq = re.sub(r\"[UZOBJ]\", \"X\", seq).upper()\n",
    "        \n",
    "        return {\n",
    "            \"id\": str(index),\n",
    "            \"primary\": seq,\n",
    "            \"protein_length\": len(seq),\n",
    "            \"targets\": self.targets[index]}\n",
    "    \n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    ''' Load data for pretrained Bert model, implemented in TAPE\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file,\n",
    "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
    "                 max_pep_len=30,\n",
    "                 train: bool = True):\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = CSVDataset(input_file,\n",
    "                               max_pep_len=max_pep_len,\n",
    "                               train=train)        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(item['primary'])\n",
    "        input_mask = np.ones_like(token_ids)\n",
    "        ret = {'input_ids': token_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': item['targets']}\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
    "        elem = batch[0]\n",
    "        batch = {key: [d[key] for d in batch] for key in elem}\n",
    "        input_ids = torch.from_numpy(tape_pad(batch['input_ids'], 0))\n",
    "        input_mask = torch.from_numpy(tape_pad(batch['input_mask'], 0))\n",
    "        tmp = np.array(batch['targets'])\n",
    "        #targets = torch.tensor(batch['targets'], dtype=torch.float32)\n",
    "        targets = torch.tensor(tmp, dtype=torch.float32)\n",
    "        ret = {'input_ids': input_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': targets}\n",
    "        \n",
    "        return ret\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - Training on 107424 samples, eval on 13428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([ 2, 20,  9, 10, 10, 13,  5, 22, 11,  5,  5, 25,  8,  5, 13, 16,  9,\n",
      "       22, 22, 10,  8, 28, 10,  8, 13,  8,  9,  5, 23, 28, 12, 25, 25, 10,\n",
      "       23, 23, 13, 19, 15, 25,  5, 15, 23, 15, 23, 22, 28, 15, 11, 15, 14,\n",
      "       27, 27, 27, 27, 27, 27, 27, 27,  3]), 'input_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'targets': array([0.698876, 1.      ])}\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "trainset = BertDataset('../../dataset/netMHCIIpan3.2/train_mini.csv', max_pep_len=24)\n",
    "valset = BertDataset('../../dataset/netMHCIIpan3.2/eval_mini.csv', max_pep_len=24)\n",
    "first_sample = trainset[0] \n",
    "#print(first_sample['input_ids']) # indices del one-hot encoding\n",
    "#print(first_sample['input_mask'])\n",
    "#print(first_sample['targets']) \n",
    "print(first_sample)\n",
    "print(first_sample['input_ids'].shape)\n",
    "\n",
    "logging.basicConfig(format='%(name)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = DataLoader(        trainset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=trainset.collate_fn)\n",
    "\n",
    "val_data = DataLoader(        valset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=valset.collate_fn)\n",
    "\n",
    "logger.info(\"Training on {0} samples, eval on {1}\".format(len(trainset), len(valset)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC not initialized from pretrained model: ['classify.classify.main.0.bias', 'classify.classify.main.0.weight_g', 'classify.classify.main.0.weight_v', 'classify.classify.main.3.bias', 'classify.classify.main.3.weight_g', 'classify.classify.main.3.weight_v']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classify): MHCHead(\n",
      "    (classify): SimpleMLP(\n",
      "      (main): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=True)\n",
      "        (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPE tiene 92356612 parametros <br>\n",
    "ProtTrans (prot_bert_bfd) tiene 419933186 (4x larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 92356612\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"params:\", pytorch_total_params)\n",
    "\n",
    "#from torchvision import models\n",
    "#from torchsummary import summary\n",
    "#summary(model, (60, 768))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC_CNN not initialized from pretrained model: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC_CNN: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC_CNN(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=36288, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC_CNN.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3357/3357 [08:39<00:00,  6.46it/s]\n",
      "root - Sample dict log: {'val_cor': 0.47600229845905223, 'val_auc': 0.8276786000804977, 'val_ap': 0.26774950394273983, 'val_mass_auc': 0.18202043838814458, 'val_loss': 0.009493345314276726, 'train_loss': 0.01896895558981401}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (inf --> -0.827679).  Saving model ...\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 150/3357 [00:23<08:24,  6.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_metrics \u001b[39m=\u001b[39m train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     eval_metrics \u001b[39m=\u001b[39m evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     eval_metrics[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_metrics\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/utils_model.py:300\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\u001b[0m\n\u001b[1;32m    298\u001b[0m label_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    299\u001b[0m mass_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m1\u001b[39m])\n\u001b[0;32m--> 300\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39msum(label_valid) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     aff_loss \u001b[39m=\u001b[39m aff_criterion(logits[label_valid, \u001b[39m0\u001b[39m], targets[label_valid, \u001b[39m0\u001b[39m])\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.15\n",
    "w_pos = 1.0 # mass positive weight\n",
    "save = \"TRAIN_4_bertmhc_model.pt\"\n",
    "alpha = 0.0 # alpha weight on mass loss, affinity loss weight with 1-alpha\n",
    "patience = 5 # Earlystopping patience\n",
    "metric = 'val_auc' # validation metric, default auc\n",
    "\n",
    "aff_criterion = nn.BCEWithLogitsLoss()\n",
    "w_pos = torch.tensor([w_pos]).to(device)\n",
    "mass_criterion = nn.BCEWithLogitsLoss(pos_weight=w_pos, reduction='none')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, min_lr=1e-4, factor=0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, saveto=save)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training epoch {}\".format(epoch))\n",
    "    train_metrics = train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n",
    "    eval_metrics = evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n",
    "    eval_metrics['train_loss'] = train_metrics\n",
    "    logs = eval_metrics\n",
    "\n",
    "    scheduler.step(logs.get(metric))\n",
    "    logging.info('Sample dict log: %s' % logs)\n",
    "\n",
    "    # callbacks\n",
    "    early_stopping(-logs.get(metric), model, optimizer)\n",
    "    if early_stopping.early_stop or logs.get(metric) <= 0:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 56, 764])\n",
      "torch.Size([32, 6, 28, 382])\n",
      "torch.Size([32, 16, 24, 378])\n",
      "torch.Size([32, 16, 12, 189])\n",
      "torch.Size([32, 36288])\n",
      "torch.Size([32, 10000])\n",
      "torch.Size([32, 500])\n",
      "tensor([[0.0018, 0.0019, 0.0023,  ..., 0.0021, 0.0021, 0.0020],\n",
      "        [0.0019, 0.0020, 0.0023,  ..., 0.0021, 0.0021, 0.0021],\n",
      "        [0.0018, 0.0020, 0.0023,  ..., 0.0021, 0.0022, 0.0020],\n",
      "        ...,\n",
      "        [0.0019, 0.0020, 0.0023,  ..., 0.0021, 0.0021, 0.0021],\n",
      "        [0.0019, 0.0020, 0.0023,  ..., 0.0021, 0.0021, 0.0021],\n",
      "        [0.0019, 0.0019, 0.0022,  ..., 0.0021, 0.0020, 0.0021]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23905/2800599566.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(n6(x))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# con una CNN 1d\n",
    "n = nn.Conv1d(60, 128, kernel_size=3)\n",
    "\n",
    "input = torch.randn(32, 60, 768) # embed de tape con batch size 32 [batch_size, dimx, dimy]\n",
    "output = n(input)\n",
    "#print(output)\n",
    "\n",
    "# con una CNN 2d\n",
    "n1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "n2 = nn.MaxPool2d(2, 2)  \n",
    "n3 = conv2 = nn.Conv2d(6, 16, 5) \n",
    "\n",
    "n4 = nn.Linear(16*12*189, 10000) \n",
    "n6 = nn.Linear(10000, 500)\n",
    "n7 = nn.Softmax()    \n",
    "\n",
    "input = torch.randn(32, 60, 768) # input desde tape model con 32 de batch size\n",
    "input = input.view(32, 1, 60, 768)\n",
    "\n",
    "#input = torch.randn(32, 1, 60, 768)\n",
    "x = F.relu(n1(input))           # [32, 1, 60, 768] -> [32, 6, 56, 764]\n",
    "print(x.shape)\n",
    "x = n2(x)                       # [32, 6, 56, 764] -> [32, 6, 28, 382]\n",
    "print(x.shape)\n",
    "x = F.relu(n3(x))               # [32, 6, 28, 382] -> [32, 16, 24, 378]\n",
    "print(x.shape)\n",
    "x = n2(x)                       # [32, 16, 24, 378] -> [32, 16, 12, 189]\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(-1, 16*12*189) \n",
    "print(x.shape)\n",
    "\n",
    "x = F.relu(n4(x))\n",
    "print(x.shape)\n",
    "output = F.softmax(n6(x))\n",
    "print(output.shape)\n",
    "#outputs = n7(x) \n",
    "#print(x.shape)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706e3f9463f4659aea008cbeb97149de47ce197374eec6ebe341c89d29bb6fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
