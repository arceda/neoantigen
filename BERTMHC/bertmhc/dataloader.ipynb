{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este script, se detalla el dataloader para preparar el training set. EL training set de ejemplo esta en el archivo train_old.csv\n",
    "De dicho archivo, se utiliza la columna 'mhc' y 'peptide' concatenadas como input y el target esta compuesto por las columnas 'label' y 'masslabel'. Tambien se utiliza un la función collate_fn de pytorch para asegurar el mismo tamaño de los inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection, Mapping\n",
    "from pathlib import Path\n",
    "from tape.tokenizers import TAPETokenizer\n",
    "from tape.datasets import pad_sequences as tape_pad\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_file: Union[str, Path, pd.DataFrame],\n",
    "                 max_pep_len=30,\n",
    "                 train: bool = True):\n",
    "        if isinstance(data_file, pd.DataFrame):\n",
    "            data = data_file\n",
    "        else:\n",
    "            data = pd.read_csv(data_file)\n",
    "        mhc = data['mhc']\n",
    "        self.mhc = mhc.values\n",
    "        peptide = data['peptide']\n",
    "        peptide = peptide.apply(lambda x: x[:max_pep_len])\n",
    "        self.peptide = peptide.values\n",
    "        if not train:\n",
    "            data['label'] = np.nan\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'masslabel' not in data and 'label' not in data:\n",
    "            raise ValueError(\"missing label.\")\n",
    "        if 'masslabel' not in data:\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'label' not in data:\n",
    "            data['label'] = np.nan\n",
    "\n",
    "        ###########################################################################################################\n",
    "        ##### el target esta compuesto por el label(float) y masslabel(int) #######################################\n",
    "        self.targets = np.stack([data['label'], data['masslabel']], axis=1)\n",
    "        self.data = data        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mhc)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        ###########################################################################################################\n",
    "        ##### aqui concatena el MHC con el peptido para que todo eso sea el input #################################\n",
    "        seq = self.mhc[index] + self.peptide[index]\n",
    "        return {\n",
    "            \"id\": str(index),\n",
    "            \"primary\": seq,\n",
    "            \"protein_length\": len(seq),\n",
    "            \"targets\": self.targets[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    ''' Load data for pretrained Bert model, implemented in TAPE\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file,\n",
    "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
    "                 max_pep_len=30,\n",
    "                 train: bool = True):\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = CSVDataset(input_file,\n",
    "                               max_pep_len=max_pep_len,\n",
    "                               train=train)        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(item['primary'])\n",
    "        input_mask = np.ones_like(token_ids)\n",
    "        ret = {'input_ids': token_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': item['targets']}\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
    "        elem = batch[0]\n",
    "        batch = {key: [d[key] for d in batch] for key in elem}\n",
    "        input_ids = torch.from_numpy(tape_pad(batch['input_ids'], 0))\n",
    "        input_mask = torch.from_numpy(tape_pad(batch['input_mask'], 0))\n",
    "        tmp = np.array(batch['targets'])\n",
    "        #targets = torch.tensor(batch['targets'], dtype=torch.float32)\n",
    "        targets = torch.tensor(tmp, dtype=torch.float32)\n",
    "        ret = {'input_ids': input_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': targets}\n",
    "        \n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 20  9 10 10 13  5 22 11  5  5 25  8  5 13 16  9 22 22 10  8 28 10  8\n",
      " 13  8  9  5 23 28 12 25 25 10 23 23 13 19 15 25  5 15 23 15 23 22 28 15\n",
      " 11 15 14  3]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0.698876 1.      ]\n"
     ]
    }
   ],
   "source": [
    "trainset = BertDataset('../../dataset/netMHCIIpan3.2/train_mini.csv', max_pep_len=24, instance_weight=False)\n",
    "valset = BertDataset('../../dataset/netMHCIIpan3.2/eval_mini.csv', max_pep_len=24, instance_weight=False)\n",
    "first_sample = trainset[0] \n",
    "print(first_sample['input_ids']) # indices del one-hot encoding\n",
    "print(first_sample['input_mask'])\n",
    "print(first_sample['targets']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - Training on 107424 samples, eval on 13428\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(name)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = DataLoader(        trainset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=trainset.collate_fn)\n",
    "\n",
    "val_data = DataLoader(        valset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=valset.collate_fn)\n",
    "\n",
    "logger.info(\"Training on {0} samples, eval on {1}\".format(len(trainset), len(valset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BERTMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC not initialized from pretrained model: ['classify.classify.main.0.bias', 'classify.classify.main.0.weight_g', 'classify.classify.main.0.weight_v', 'classify.classify.main.3.bias', 'classify.classify.main.3.weight_g', 'classify.classify.main.3.weight_v']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bertmhc import BERTMHC\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils_model import EarlyStopping\n",
    "from utils_model import train, evaluate\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPE tiene 92356612 parametros <br>\n",
    "ProtTrans (prot_bert_bfd) tiene 419933186 (4x larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92356612\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3357/3357 [08:12<00:00,  6.81it/s]\n",
      "root - Sample dict log: {'val_cor': 0.5033360818216253, 'val_auc': 0.8437104698380997, 'val_ap': 0.28102128923439507, 'val_mass_auc': 0.2372045758505762, 'val_loss': 0.009377251796719572, 'train_loss': 0.018801003750574564}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (inf --> -0.843710).  Saving model ...\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 91/3357 [00:14<08:37,  6.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_metrics \u001b[39m=\u001b[39m train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     eval_metrics \u001b[39m=\u001b[39m evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     eval_metrics[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_metrics\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/utils_model.py:318\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scheduler, LambdaLR):\n\u001b[1;32m    316\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 318\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    320\u001b[0m \u001b[39mreturn\u001b[39;00m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_data\u001b[39m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.15\n",
    "w_pos = 1.0 # mass positive weight\n",
    "save = \"bertmhc_model.pt\"\n",
    "alpha = 0.0 # alpha weight on mass loss, affinity loss weight with 1-alpha\n",
    "patience = 5 # Earlystopping patience\n",
    "metric = 'val_auc' # validation metric, default auc\n",
    "\n",
    "aff_criterion = nn.BCEWithLogitsLoss()\n",
    "w_pos = torch.tensor([w_pos]).to(device)\n",
    "mass_criterion = nn.BCEWithLogitsLoss(pos_weight=w_pos, reduction='none')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, min_lr=1e-4, factor=0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, saveto=save)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training epoch {}\".format(epoch))\n",
    "    train_metrics = train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n",
    "    eval_metrics = evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n",
    "    eval_metrics['train_loss'] = train_metrics\n",
    "    logs = eval_metrics\n",
    "\n",
    "    scheduler.step(logs.get(metric))\n",
    "    logging.info('Sample dict log: %s' % logs)\n",
    "\n",
    "    # callbacks\n",
    "    early_stopping(-logs.get(metric), model, optimizer)\n",
    "    if early_stopping.early_stop or logs.get(metric) <= 0:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ImmunoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bertmhc import TAPEBackbone\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils_model import EarlyStopping\n",
    "from utils_model import train, evaluate\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = TAPEBackbone.from_pretrained('bert-base')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91961856\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3357 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'token_type_ids' and 'position_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_metrics \u001b[39m=\u001b[39m train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     eval_metrics \u001b[39m=\u001b[39m evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     eval_metrics[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_metrics\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/utils_model.py:296\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\u001b[0m\n\u001b[1;32m    293\u001b[0m instance_weights \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39minstance_weights\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    294\u001b[0m                              torch\u001b[39m.\u001b[39mones(batch[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], )\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m    295\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 296\u001b[0m logits, targets \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m    298\u001b[0m label_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    299\u001b[0m mass_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'token_type_ids' and 'position_ids'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.15\n",
    "w_pos = 1.0 # mass positive weight\n",
    "save = \"bertmhc_model.pt\"\n",
    "alpha = 0.0 # alpha weight on mass loss, affinity loss weight with 1-alpha\n",
    "patience = 5 # Earlystopping patience\n",
    "metric = 'val_auc' # validation metric, default auc\n",
    "\n",
    "aff_criterion = nn.BCEWithLogitsLoss()\n",
    "w_pos = torch.tensor([w_pos]).to(device)\n",
    "mass_criterion = nn.BCEWithLogitsLoss(pos_weight=w_pos, reduction='none')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, min_lr=1e-4, factor=0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, saveto=save)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training epoch {}\".format(epoch))\n",
    "    train_metrics = train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n",
    "    eval_metrics = evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n",
    "    eval_metrics['train_loss'] = train_metrics\n",
    "    logs = eval_metrics\n",
    "\n",
    "    scheduler.step(logs.get(metric))\n",
    "    logging.info('Sample dict log: %s' % logs)\n",
    "\n",
    "    # callbacks\n",
    "    early_stopping(-logs.get(metric), model, optimizer)\n",
    "    if early_stopping.early_stop or logs.get(metric) <= 0:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706e3f9463f4659aea008cbeb97149de47ce197374eec6ebe341c89d29bb6fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
