{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALOADER\n",
    "\n",
    "En este script, se detalla el dataloader para preparar el training set. EL training set de ejemplo esta en el archivo train_old.csv\n",
    "De dicho archivo, se utiliza la columna 'mhc' y 'peptide' concatenadas como input y el target esta compuesto por las columnas 'label' y 'masslabel'. Tambien se utiliza un la función collate_fn de pytorch para asegurar el mismo tamaño de los inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection, Mapping\n",
    "from pathlib import Path\n",
    "from tape.tokenizers import TAPETokenizer\n",
    "from tape.datasets import pad_sequences as tape_pad\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_file: Union[str, Path, pd.DataFrame],\n",
    "                 max_pep_len=30,\n",
    "                 train: bool = True):\n",
    "        if isinstance(data_file, pd.DataFrame):\n",
    "            data = data_file\n",
    "        else:\n",
    "            data = pd.read_csv(data_file)\n",
    "        mhc = data['mhc']\n",
    "        self.mhc = mhc.values\n",
    "        peptide = data['peptide']\n",
    "        peptide = peptide.apply(lambda x: x[:max_pep_len])\n",
    "        self.peptide = peptide.values\n",
    "        if not train:\n",
    "            data['label'] = np.nan\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'masslabel' not in data and 'label' not in data:\n",
    "            raise ValueError(\"missing label.\")\n",
    "        if 'masslabel' not in data:\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'label' not in data:\n",
    "            data['label'] = np.nan\n",
    "\n",
    "        ###########################################################################################################\n",
    "        ##### el target esta compuesto por el label(float) y masslabel(int) #######################################\n",
    "        self.targets = np.stack([data['label'], data['masslabel']], axis=1)\n",
    "        self.data = data\n",
    "        if 'instance_weights' in data:\n",
    "            self.instance_weights = data['instance_weights'].values\n",
    "        else:\n",
    "            self.instance_weights = np.ones(data.shape[0],)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mhc)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        ###########################################################################################################\n",
    "        ##### aqui concatena el MHC con el peptido para que todo eso sea el input #################################\n",
    "        seq = self.mhc[index] + self.peptide[index]\n",
    "        return {\n",
    "            \"id\": str(index),\n",
    "            \"primary\": seq,\n",
    "            \"protein_length\": len(seq),\n",
    "            \"targets\": self.targets[index],\n",
    "            \"instance_weights\": self.instance_weights[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    ''' Load data for pretrained Bert model, implemented in TAPE\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file,\n",
    "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
    "                 max_pep_len=30,\n",
    "                 in_memory: bool = False,\n",
    "                 instance_weight: bool = False,\n",
    "                 train: bool = True):\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = CSVDataset(input_file,\n",
    "                               max_pep_len=max_pep_len,\n",
    "                               train=train)\n",
    "        self.instance_weight = instance_weight\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        #print(item['primary']) # input\n",
    "        #print(item['targets']) # target\n",
    "        token_ids = self.tokenizer.encode(item['primary'])\n",
    "        input_mask = np.ones_like(token_ids)\n",
    "        ret = {'input_ids': token_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': item['targets']}\n",
    "        if self.instance_weight:\n",
    "            ret['instance_weights'] = item['instance_weights']\n",
    "        return ret\n",
    "\n",
    "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
    "        elem = batch[0]\n",
    "        batch = {key: [d[key] for d in batch] for key in elem}\n",
    "        input_ids = torch.from_numpy(tape_pad(batch['input_ids'], 0))\n",
    "        input_mask = torch.from_numpy(tape_pad(batch['input_mask'], 0))\n",
    "        tmp = np.array(batch['targets'])\n",
    "        #targets = torch.tensor(batch['targets'], dtype=torch.float32)\n",
    "        targets = torch.tensor(tmp, dtype=torch.float32)\n",
    "        ret = {'input_ids': input_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': targets}\n",
    "        if self.instance_weight:\n",
    "            instance_weights = torch.tensor(batch['instance_weights'],\n",
    "                                            dtype=torch.float32)\n",
    "            ret['instance_weights'] = instance_weights\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  9 22 11 22 12  7  9 22 28 13 23 21 23 12 16 13 23 28  8 12 25 11 22\n",
      " 19 12 13 23 27  8 27 26 17 12 22 13 12 15 25 13 12 21 13 21 23 15 13 11\n",
      " 20  9  3]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0.75609 1.     ]\n"
     ]
    }
   ],
   "source": [
    "trainset = BertDataset('../tests/data/train_old.csv', max_pep_len=24, instance_weight=False)\n",
    "first_sample = trainset[0] \n",
    "print(first_sample['input_ids']) # indices del one-hot encoding\n",
    "print(first_sample['input_mask'])\n",
    "print(first_sample['targets']) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2,  9, 22, 11, 22, 12,  7,  9, 22, 28, 13, 23, 21, 23, 12, 16, 13, 23,\n",
      "         28,  8, 12, 25, 11, 22, 19, 12, 13, 23, 27,  8, 27, 26, 17, 12, 22, 13,\n",
      "         12, 15, 25, 13, 12, 21, 13, 21, 23, 15, 13, 11, 20,  9,  3,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 2,  9, 22, 11, 22, 12,  7,  9, 22, 28, 13, 23, 21, 23, 12, 16, 13, 23,\n",
      "         28,  8, 12, 25, 11, 22, 19, 12, 13, 23, 27,  8, 27, 26, 17, 12, 22,  5,\n",
      "          5, 15, 15, 25, 25,  5, 25, 11, 15, 21, 25, 25,  7,  5, 14, 28,  5, 15,\n",
      "          5,  3],\n",
      "        [ 2,  9, 22, 11, 22, 12,  7,  9, 22, 28, 13, 23, 21, 23, 12, 16, 13, 23,\n",
      "         28,  8, 12, 25, 11, 22, 19, 12, 13, 23, 27,  8, 27, 26, 17, 12, 22, 15,\n",
      "         22, 25, 23,  9, 20, 22,  9, 10, 28, 10, 19, 21,  5, 19,  3,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 2,  9, 22, 11, 22, 12,  7,  9, 22, 28, 13, 23, 21, 23, 12, 16, 13, 23,\n",
      "         28,  8, 12, 25, 11, 22, 19, 12, 13, 23, 27,  8, 27, 26, 17, 12, 22, 10,\n",
      "         10, 23,  9, 15,  8, 11, 25, 21, 15, 12, 21, 10,  5, 19, 19,  7, 14, 19,\n",
      "         15,  3]]), 'input_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'targets': tensor([[0.7561, 1.0000],\n",
      "        [0.3472, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.3184, 0.0000]])}\n"
     ]
    }
   ],
   "source": [
    "train_data = DataLoader(        trainset,\n",
    "                                batch_size=4,\n",
    "                                #shuffle=True,\n",
    "                                num_workers=16,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=trainset.collate_fn)\n",
    "\n",
    "\n",
    "print(next(iter(train_data))) # obtenemos el primer batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706e3f9463f4659aea008cbeb97149de47ce197374eec6ebe341c89d29bb6fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
