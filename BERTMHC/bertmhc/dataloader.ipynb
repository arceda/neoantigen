{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este script, se detalla el dataloader para preparar el training set. EL training set de ejemplo esta en el archivo train_old.csv\n",
    "De dicho archivo, se utiliza la columna 'mhc' y 'peptide' concatenadas como input y el target esta compuesto por las columnas 'label' y 'masslabel'. Tambien se utiliza un la función collate_fn de pytorch para asegurar el mismo tamaño de los inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "__main__ - Training on 107424 samples, eval on 13428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([ 2, 20,  9, 10, 10, 13,  5, 22, 11,  5,  5, 25,  8,  5, 13, 16,  9,\n",
      "       22, 22, 10,  8, 28, 10,  8, 13,  8,  9,  5, 23, 28, 12, 25, 25, 10,\n",
      "       23, 23, 13, 19, 15, 25,  5, 15, 23, 15, 23, 22, 28, 15, 11, 15, 14,\n",
      "        3]), 'input_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1]), 'targets': array([0.698876, 1.      ])}\n",
      "(52,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection, Mapping\n",
    "from pathlib import Path\n",
    "from tape.tokenizers import TAPETokenizer\n",
    "from tape.datasets import pad_sequences as tape_pad\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bertmhc import BERTMHC, BERTMHC_CNN\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils_model import EarlyStopping\n",
    "from utils_model import train, evaluate\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_file: Union[str, Path, pd.DataFrame],\n",
    "                 max_pep_len=60,\n",
    "                 train: bool = True):\n",
    "        if isinstance(data_file, pd.DataFrame):\n",
    "            data = data_file\n",
    "        else:\n",
    "            data = pd.read_csv(data_file)\n",
    "        mhc = data['mhc']\n",
    "        self.mhc = mhc.values\n",
    "        peptide = data['peptide']\n",
    "        peptide = peptide.apply(lambda x: x[:max_pep_len])\n",
    "        self.peptide = peptide.values\n",
    "        if not train:\n",
    "            data['label'] = np.nan\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'masslabel' not in data and 'label' not in data:\n",
    "            raise ValueError(\"missing label.\")\n",
    "        if 'masslabel' not in data:\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'label' not in data:\n",
    "            data['label'] = np.nan\n",
    "\n",
    "        ###########################################################################################################\n",
    "        ##### el target esta compuesto por el label(float) y masslabel(int) #######################################\n",
    "        self.targets = np.stack([data['label'], data['masslabel']], axis=1)\n",
    "        self.data = data        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mhc)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        ###########################################################################################################\n",
    "        ##### aqui concatena el MHC con el peptido para que todo eso sea el input #################################\n",
    "        seq = self.mhc[index] + self.peptide[index]\n",
    "        \n",
    "        # aqui hacemos padding y reemplazamos algunos aminoacidos\n",
    "        #seq = seq + 'X' * (58 - len(seq)) \n",
    "        #seq = re.sub(r\"[UZOBJ]\", \"X\", seq).upper()\n",
    "        \n",
    "        return {\n",
    "            \"id\": str(index),\n",
    "            \"primary\": seq,\n",
    "            \"protein_length\": len(seq),\n",
    "            \"targets\": self.targets[index]}\n",
    "    \n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    ''' Load data for pretrained Bert model, implemented in TAPE\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file,\n",
    "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
    "                 max_pep_len=30,\n",
    "                 train: bool = True):\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = CSVDataset(input_file,\n",
    "                               max_pep_len=max_pep_len,\n",
    "                               train=train)        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(item['primary'])\n",
    "        input_mask = np.ones_like(token_ids)\n",
    "        ret = {'input_ids': token_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': item['targets']}\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
    "        elem = batch[0]\n",
    "        batch = {key: [d[key] for d in batch] for key in elem}\n",
    "        input_ids = torch.from_numpy(tape_pad(batch['input_ids'], 0))\n",
    "        input_mask = torch.from_numpy(tape_pad(batch['input_mask'], 0))\n",
    "        tmp = np.array(batch['targets'])\n",
    "        #targets = torch.tensor(batch['targets'], dtype=torch.float32)\n",
    "        targets = torch.tensor(tmp, dtype=torch.float32)\n",
    "        ret = {'input_ids': input_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': targets}\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "# dataset\n",
    "trainset = BertDataset('../../dataset/netMHCIIpan3.2/train_mini.csv', max_pep_len=24)\n",
    "valset = BertDataset('../../dataset/netMHCIIpan3.2/eval_mini.csv', max_pep_len=24)\n",
    "first_sample = trainset[0] \n",
    "#print(first_sample['input_ids']) # indices del one-hot encoding\n",
    "#print(first_sample['input_mask'])\n",
    "#print(first_sample['targets']) \n",
    "print(first_sample)\n",
    "print(first_sample['input_ids'].shape)\n",
    "\n",
    "logging.basicConfig(format='%(name)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = DataLoader(        trainset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=trainset.collate_fn)\n",
    "\n",
    "val_data = DataLoader(        valset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=valset.collate_fn)\n",
    "\n",
    "logger.info(\"Training on {0} samples, eval on {1}\".format(len(trainset), len(valset)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC not initialized from pretrained model: ['classify.classify.main.0.bias', 'classify.classify.main.0.weight_g', 'classify.classify.main.0.weight_v', 'classify.classify.main.3.bias', 'classify.classify.main.3.weight_g', 'classify.classify.main.3.weight_v']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classify): MHCHead(\n",
      "    (classify): SimpleMLP(\n",
      "      (main): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=True)\n",
      "        (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPE tiene 92356612 parametros <br>\n",
    "ProtTrans (prot_bert_bfd) tiene 419933186 (4x larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 92356612\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"params:\", pytorch_total_params)\n",
    "\n",
    "#from torchvision import models\n",
    "#from torchsummary import summary\n",
    "#summary(model, (60, 768))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC_CNN not initialized from pretrained model: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC_CNN: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC_CNN(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=36288, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC_CNN.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bertmhc import BERTMHC_LINEAR, BERTMHC\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "#model = BERTMHC_LINEAR.from_pretrained('bert-base')\n",
    "model = BERTMHC_LINEAR.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC RNN ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC_RNN_ATT not initialized from pretrained model: ['w_omega', 'u_omega', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'rnn.weight_ih_l0_reverse', 'rnn.weight_hh_l0_reverse', 'rnn.bias_ih_l0_reverse', 'rnn.bias_hh_l0_reverse', 'rnn.weight_ih_l1', 'rnn.weight_hh_l1', 'rnn.bias_ih_l1', 'rnn.bias_hh_l1', 'rnn.weight_ih_l1_reverse', 'rnn.weight_hh_l1_reverse', 'rnn.bias_ih_l1_reverse', 'rnn.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC_RNN_ATT: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC_RNN_ATT(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (rnn): LSTM(768, 768, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (att_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1536, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from bertmhc import BERTMHC_RNN_ATT\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "#model = BERTMHC_LINEAR.from_pretrained('bert-base')\n",
    "model = BERTMHC_RNN_ATT.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3357 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_metrics \u001b[39m=\u001b[39m train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     eval_metrics \u001b[39m=\u001b[39m evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     eval_metrics[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_metrics\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/utils_model.py:296\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\u001b[0m\n\u001b[1;32m    293\u001b[0m instance_weights \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39minstance_weights\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    294\u001b[0m                              torch\u001b[39m.\u001b[39mones(batch[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], )\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m    295\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 296\u001b[0m logits, targets \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m    298\u001b[0m label_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    299\u001b[0m mass_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/bertmhc.py:163\u001b[0m, in \u001b[0;36mBERTMHC_RNN_ATT.forward\u001b[0;34m(self, input_ids, input_mask, targets)\u001b[0m\n\u001b[1;32m    161\u001b[0m rnn_out, (ht, ct) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(pooled_output)\n\u001b[1;32m    162\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_dropout(rnn_out)\n\u001b[0;32m--> 163\u001b[0m attn_output, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_net(rnn_out, query)  \u001b[39m# \u001b[39;00m\n\u001b[1;32m    164\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(attn_output)\n\u001b[1;32m    166\u001b[0m out \u001b[39m=\u001b[39m (logits, )\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/bertmhc.py:148\u001b[0m, in \u001b[0;36mBERTMHC_RNN_ATT.attention_net\u001b[0;34m(self, x, query, mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattention_net\u001b[39m(\u001b[39mself\u001b[39m, x, query, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    147\u001b[0m     d_k \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m     scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query, x\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)) \u001b[39m/\u001b[39mmath\u001b[39m.\u001b[39msqrt(d_k)  \u001b[39m#   scores:[batch, seq_len, seq_len]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     p_attn \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m     rattention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(p_attn, x)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.15\n",
    "w_pos = 1.0 # mass positive weight\n",
    "save = \"TRAIN_x_bertmhc_model.pt\"\n",
    "alpha = 0.0 # alpha weight on mass loss, affinity loss weight with 1-alpha\n",
    "patience = 5 # Earlystopping patience\n",
    "metric = 'val_auc' # validation metric, default auc\n",
    "\n",
    "aff_criterion = nn.BCEWithLogitsLoss()\n",
    "w_pos = torch.tensor([w_pos]).to(device)\n",
    "mass_criterion = nn.BCEWithLogitsLoss(pos_weight=w_pos, reduction='none')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, min_lr=1e-4, factor=0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, saveto=save)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training epoch {}\".format(epoch))\n",
    "    train_metrics = train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n",
    "    eval_metrics = evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n",
    "    eval_metrics['train_loss'] = train_metrics\n",
    "    logs = eval_metrics\n",
    "\n",
    "    scheduler.step(logs.get(metric))\n",
    "    logging.info('Sample dict log: %s' % logs)\n",
    "\n",
    "    # callbacks\n",
    "    early_stopping(-logs.get(metric), model, optimizer)\n",
    "    if early_stopping.early_stop or logs.get(metric) <= 0:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate dimensiones de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 56, 764])\n",
      "torch.Size([32, 6, 28, 382])\n",
      "torch.Size([32, 16, 24, 378])\n",
      "torch.Size([32, 16, 12, 189])\n",
      "torch.Size([32, 36288])\n",
      "torch.Size([32, 10000])\n",
      "torch.Size([32, 500])\n",
      "tensor([[0.0018, 0.0019, 0.0023,  ..., 0.0021, 0.0021, 0.0020],\n",
      "        [0.0019, 0.0020, 0.0023,  ..., 0.0021, 0.0021, 0.0021],\n",
      "        [0.0018, 0.0020, 0.0023,  ..., 0.0021, 0.0022, 0.0020],\n",
      "        ...,\n",
      "        [0.0019, 0.0020, 0.0023,  ..., 0.0021, 0.0021, 0.0021],\n",
      "        [0.0019, 0.0020, 0.0023,  ..., 0.0021, 0.0021, 0.0021],\n",
      "        [0.0019, 0.0019, 0.0022,  ..., 0.0021, 0.0020, 0.0021]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23905/2800599566.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(n6(x))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# con una CNN 1d\n",
    "n = nn.Conv1d(60, 128, kernel_size=3)\n",
    "\n",
    "input = torch.randn(32, 60, 768) # embed de tape con batch size 32 [batch_size, dimx, dimy]\n",
    "output = n(input)\n",
    "#print(output)\n",
    "\n",
    "# con una CNN 2d\n",
    "n1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "n2 = nn.MaxPool2d(2, 2)  \n",
    "n3 = conv2 = nn.Conv2d(6, 16, 5) \n",
    "\n",
    "n4 = nn.Linear(16*12*189, 10000) \n",
    "n6 = nn.Linear(10000, 500)\n",
    "n7 = nn.Softmax()    \n",
    "\n",
    "input = torch.randn(32, 60, 768) # input desde tape model con 32 de batch size\n",
    "input = input.view(32, 1, 60, 768)\n",
    "\n",
    "#input = torch.randn(32, 1, 60, 768)\n",
    "x = F.relu(n1(input))           # [32, 1, 60, 768] -> [32, 6, 56, 764]\n",
    "print(x.shape)\n",
    "x = n2(x)                       # [32, 6, 56, 764] -> [32, 6, 28, 382]\n",
    "print(x.shape)\n",
    "x = F.relu(n3(x))               # [32, 6, 28, 382] -> [32, 16, 24, 378]\n",
    "print(x.shape)\n",
    "x = n2(x)                       # [32, 16, 24, 378] -> [32, 16, 12, 189]\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(-1, 16*12*189) \n",
    "print(x.shape)\n",
    "\n",
    "x = F.relu(n4(x))\n",
    "print(x.shape)\n",
    "output = F.softmax(n6(x))\n",
    "print(output.shape)\n",
    "#outputs = n7(x) \n",
    "#print(x.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1])\n",
      "tensor([[[0.5482]],\n",
      "\n",
      "        [[0.4293]],\n",
      "\n",
      "        [[0.5503]],\n",
      "\n",
      "        [[0.4340]],\n",
      "\n",
      "        [[0.6277]],\n",
      "\n",
      "        [[0.5838]],\n",
      "\n",
      "        [[0.4624]],\n",
      "\n",
      "        [[0.6540]],\n",
      "\n",
      "        [[0.6292]],\n",
      "\n",
      "        [[0.8801]],\n",
      "\n",
      "        [[0.1949]],\n",
      "\n",
      "        [[0.4478]],\n",
      "\n",
      "        [[0.7024]],\n",
      "\n",
      "        [[0.7735]],\n",
      "\n",
      "        [[0.2155]],\n",
      "\n",
      "        [[0.3104]],\n",
      "\n",
      "        [[0.6967]],\n",
      "\n",
      "        [[0.4506]],\n",
      "\n",
      "        [[0.4824]],\n",
      "\n",
      "        [[0.4654]],\n",
      "\n",
      "        [[0.4646]],\n",
      "\n",
      "        [[0.4440]],\n",
      "\n",
      "        [[0.4283]],\n",
      "\n",
      "        [[0.4069]],\n",
      "\n",
      "        [[0.5271]],\n",
      "\n",
      "        [[0.4635]],\n",
      "\n",
      "        [[0.7084]],\n",
      "\n",
      "        [[0.3120]],\n",
      "\n",
      "        [[0.4717]],\n",
      "\n",
      "        [[0.5794]],\n",
      "\n",
      "        [[0.4974]],\n",
      "\n",
      "        [[0.3781]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dropout     = nn.Dropout(0.2)\n",
    "linear      = nn.Linear(768, 1)\n",
    "sigmoid     = nn.Sigmoid()    \n",
    "\n",
    "input = torch.randn(32, 1, 768) \n",
    "\n",
    "out = dropout(input)   \n",
    "out = linear(out) \n",
    "out = sigmoid(out)         \n",
    "\n",
    "print(out.shape)\n",
    "print(out)\n",
    "\n",
    "from tape.models.modeling_utils import SimpleMLP\n",
    "logits = self.classify(average, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils_model import EarlyStopping, MAData\n",
    "from tape import ProteinBertConfig\n",
    "from dataloader import BertDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from bertmhc import BERTMHC\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils_model import train, evaluate\n",
    "\n",
    "def predict(data, model_path, output_path):\n",
    "    inp = data\n",
    "    config = ProteinBertConfig.from_pretrained('bert-base')\n",
    "    model = BERTMHC(config)\n",
    "    weights = torch.load(model_path)\n",
    "    \n",
    "    if list(weights.keys())[0].startswith('module.'):\n",
    "        weights = {k[7:]: v for k, v in weights.items() if k.startswith('module.')}\n",
    "    model.load_state_dict(weights)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    valset = BertDataset(inp,\n",
    "                         max_pep_len=24,\n",
    "                         train=False)\n",
    "    val_data = DataLoader(valset,\n",
    "                          batch_size=16,\n",
    "                          num_workers=16,\n",
    "                          pin_memory=True,\n",
    "                          collate_fn=valset.collate_fn)\n",
    "    pred = []\n",
    "    for batch in tqdm(val_data):\n",
    "        batch = {name: tensor.to(device)\n",
    "                 for name, tensor in batch.items()}\n",
    "        logits, _ = model(**batch)\n",
    "        pred.append(torch.sigmoid(logits).cpu().detach().numpy())\n",
    "    dt = pd.read_csv(inp)\n",
    "    pred = np.concatenate(pred)\n",
    "    \n",
    "    '''if args.task == 'binding':\n",
    "        dt['bertmhc_pred'] = pred[:,0]\n",
    "    else:\n",
    "        dt['bertmhc_pred'] = pred[:,1]'''\n",
    "    \n",
    "    dt['bertmhc_pred'] = pred[:,0]\n",
    "    dt.to_csv(output_path, index=None)\n",
    "    return dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict del modelo TRAIN_3 de la forma igual a BERTMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [00:18<00:00, 45.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dt_train_3 = predict(    data        =   \"../../dataset/netMHCIIpan3.2/test_mini.csv\", \n",
    "            model_path  =   \"TRAIN_3_bertmhc_model.pt\", \n",
    "            output_path =   \"train_3_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.8066125549184601, 'precision': 0.8008178687638494, 'recall': 0.8001206157435652, 'fscore': 0.8004603694779975}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6595, 1277],\n",
       "       [1320, 4237]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "dt_train_3['pred'] = dt_train_3.apply(lambda row: (1 if row['bertmhc_pred'] > 0.426 else 0), axis=1)    # 0.807952937672202\n",
    "#dt_train_3['pred'] = dt_train_3.apply(lambda row: (1 if row['bertmhc_pred'] > 0.5 else 0), axis=1)     # 0.7962618214312309\n",
    "\n",
    "y_true_3 = dt_train_3['masslabel'].to_numpy()\n",
    "y_pred_3 = dt_train_3['pred'].to_numpy()\n",
    "acc = accuracy_score(y_true_3, y_pred_3)\n",
    "precision, recall, fscore, _ =  precision_recall_fscore_support(y_true_3, y_pred_3, average='macro')\n",
    "result_3 = {\"acc\":acc, \"precision\":precision, \"recall\":recall, \"fscore\":fscore }\n",
    "print(result_3)\n",
    "confusion_matrix(y_true_3, y_pred_3)\n",
    "#print(accuracy_score(y_true, y_pred, normalize=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict del modelo TRAIN_4 de la forma igual a BERTMHC pero con padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [00:18<00:00, 45.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# tenemos q entrear una vez mas train_4\n",
    "dt_train_4 = predict(    data        =   \"../../dataset/netMHCIIpan3.2/test_mini.csv\", \n",
    "            model_path  =   \"TRAIN_4_bertmhc_model.pt\", \n",
    "            output_path =   \"train_4_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.7375083773922109, 'precision': 0.7352630171197421, 'recall': 0.7147436407387737, 'fscore': 0.7192116524243642}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6666, 1206],\n",
       "       [2319, 3238]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "dt_train_4['pred'] = dt_train_4.apply(lambda row: (1 if row['bertmhc_pred'] > 0.426 else 0), axis=1)    # 0.7375083773922109\n",
    "#dt_train_4['pred'] = dt_train_4.apply(lambda row: (1 if row['bertmhc_pred'] > 0.5 else 0), axis=1)       # 0.7178494303373296\n",
    "\n",
    "y_true_4 = dt_train_4['masslabel'].to_numpy()\n",
    "y_pred_4 = dt_train_4['pred'].to_numpy()\n",
    "acc = accuracy_score(y_true_4, y_pred_4)\n",
    "precision, recall, fscore, _ =  precision_recall_fscore_support(y_true_4, y_pred_4, average='macro')\n",
    "result_4 = {\"acc\":acc, \"precision\":precision, \"recall\":recall, \"fscore\":fscore }\n",
    "print(result_4)\n",
    "confusion_matrix(y_true_4, y_pred_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr_3, tpr_3, _ = metrics.roc_curve(y_true_3,  y_pred_3)\n",
    "auc_3 = metrics.roc_auc_score(y_true_3, y_pred_3)\n",
    "\n",
    "fpr_4, tpr_4, _ = metrics.roc_curve(y_true_4,  y_pred_4)\n",
    "auc_4 = metrics.roc_auc_score(y_true_4, y_pred_4)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr_3,tpr_3, label=\"BERTMHC, auc=\"+str(round(auc_3,3)))\n",
    "plt.plot(fpr_4,tpr_4, label=\"BERTMHC-pad, auc=\"+str(round(auc_4, 3)))\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "#plt.show()\n",
    "plt.savefig('roc_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig = plt.subplots(figsize =(12, 8))\n",
    " \n",
    "# set height of bar\n",
    "IT = list(result_3.values())\n",
    "ECE = list(result_4.values())\n",
    " \n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(IT))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(br1, IT, color ='dodgerblue', width = barWidth, label ='BERTMHC')\n",
    "plt.bar(br2, ECE, color ='lightgreen', width = barWidth, label ='BERTMHC-pad')\n",
    " \n",
    "# Adding Xticks\n",
    "plt.xlabel('Metrics',  fontsize = 15)\n",
    "plt.ylabel('Values', fontsize = 15)\n",
    "plt.xticks([r + barWidth for r in range(len(IT))],\n",
    "        ['Accuracy', 'Precision', 'Recall', 'f-score'])\n",
    " \n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('metrics_comparison.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706e3f9463f4659aea008cbeb97149de47ce197374eec6ebe341c89d29bb6fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
