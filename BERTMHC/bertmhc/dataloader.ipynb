{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este script, se detalla el dataloader para preparar el training set. EL training set de ejemplo esta en el archivo train_old.csv\n",
    "De dicho archivo, se utiliza la columna 'mhc' y 'peptide' concatenadas como input y el target esta compuesto por las columnas 'label' y 'masslabel'. Tambien se utiliza un la función collate_fn de pytorch para asegurar el mismo tamaño de los inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection, Mapping\n",
    "from pathlib import Path\n",
    "from tape.tokenizers import TAPETokenizer\n",
    "from tape.datasets import pad_sequences as tape_pad\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bertmhc import BERTMHC, BERTMHC_CNN\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils_model import EarlyStopping\n",
    "from utils_model import train, evaluate\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_file: Union[str, Path, pd.DataFrame],\n",
    "                 max_pep_len=60,\n",
    "                 train: bool = True):\n",
    "        if isinstance(data_file, pd.DataFrame):\n",
    "            data = data_file\n",
    "        else:\n",
    "            data = pd.read_csv(data_file)\n",
    "        mhc = data['mhc']\n",
    "        self.mhc = mhc.values\n",
    "        peptide = data['peptide']\n",
    "        peptide = peptide.apply(lambda x: x[:max_pep_len])\n",
    "        self.peptide = peptide.values\n",
    "        if not train:\n",
    "            data['label'] = np.nan\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'masslabel' not in data and 'label' not in data:\n",
    "            raise ValueError(\"missing label.\")\n",
    "        if 'masslabel' not in data:\n",
    "            data['masslabel'] = np.nan\n",
    "        if 'label' not in data:\n",
    "            data['label'] = np.nan\n",
    "\n",
    "        ###########################################################################################################\n",
    "        ##### el target esta compuesto por el label(float) y masslabel(int) #######################################\n",
    "        self.targets = np.stack([data['label'], data['masslabel']], axis=1)\n",
    "        self.data = data        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.mhc)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        ###########################################################################################################\n",
    "        ##### aqui concatena el MHC con el peptido para que todo eso sea el input #################################\n",
    "        seq = self.mhc[index] + self.peptide[index]\n",
    "        \n",
    "        # aqui hacemos padding y reemplazamos algunos aminoacidos\n",
    "        seq = seq + 'X' * (58 - len(seq)) \n",
    "        seq = re.sub(r\"[UZOBJ]\", \"X\", seq).upper()\n",
    "        \n",
    "        return {\n",
    "            \"id\": str(index),\n",
    "            \"primary\": seq,\n",
    "            \"protein_length\": len(seq),\n",
    "            \"targets\": self.targets[index]}\n",
    "    \n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    ''' Load data for pretrained Bert model, implemented in TAPE\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file,\n",
    "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
    "                 max_pep_len=30,\n",
    "                 train: bool = True):\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = CSVDataset(input_file,\n",
    "                               max_pep_len=max_pep_len,\n",
    "                               train=train)        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(item['primary'])\n",
    "        input_mask = np.ones_like(token_ids)\n",
    "        ret = {'input_ids': token_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': item['targets']}\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def collate_fn(self, batch) -> Dict[str, torch.Tensor]:\n",
    "        elem = batch[0]\n",
    "        batch = {key: [d[key] for d in batch] for key in elem}\n",
    "        input_ids = torch.from_numpy(tape_pad(batch['input_ids'], 0))\n",
    "        input_mask = torch.from_numpy(tape_pad(batch['input_mask'], 0))\n",
    "        tmp = np.array(batch['targets'])\n",
    "        #targets = torch.tensor(batch['targets'], dtype=torch.float32)\n",
    "        targets = torch.tensor(tmp, dtype=torch.float32)\n",
    "        ret = {'input_ids': input_ids,\n",
    "               'input_mask': input_mask,\n",
    "               'targets': targets}\n",
    "        \n",
    "        return ret\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - Training on 107424 samples, eval on 13428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([ 2, 20,  9, 10, 10, 13,  5, 22, 11,  5,  5, 25,  8,  5, 13, 16,  9,\n",
      "       22, 22, 10,  8, 28, 10,  8, 13,  8,  9,  5, 23, 28, 12, 25, 25, 10,\n",
      "       23, 23, 13, 19, 15, 25,  5, 15, 23, 15, 23, 22, 28, 15, 11, 15, 14,\n",
      "       27, 27, 27, 27, 27, 27, 27, 27,  3]), 'input_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'targets': array([0.698876, 1.      ])}\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "trainset = BertDataset('../../dataset/netMHCIIpan3.2/train_mini.csv', max_pep_len=24)\n",
    "valset = BertDataset('../../dataset/netMHCIIpan3.2/eval_mini.csv', max_pep_len=24)\n",
    "first_sample = trainset[0] \n",
    "#print(first_sample['input_ids']) # indices del one-hot encoding\n",
    "#print(first_sample['input_mask'])\n",
    "#print(first_sample['targets']) \n",
    "print(first_sample)\n",
    "print(first_sample['input_ids'].shape)\n",
    "\n",
    "logging.basicConfig(format='%(name)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = DataLoader(        trainset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=16,\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=trainset.collate_fn)\n",
    "\n",
    "val_data = DataLoader(        valset,\n",
    "                              batch_size=64,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=True,\n",
    "                              collate_fn=valset.collate_fn)\n",
    "\n",
    "logger.info(\"Training on {0} samples, eval on {1}\".format(len(trainset), len(valset)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC not initialized from pretrained model: ['classify.classify.main.0.bias', 'classify.classify.main.0.weight_g', 'classify.classify.main.0.weight_v', 'classify.classify.main.3.bias', 'classify.classify.main.3.weight_g', 'classify.classify.main.3.weight_v']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classify): MHCHead(\n",
      "    (classify): SimpleMLP(\n",
      "      (main): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.0, inplace=True)\n",
      "        (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros de BERTMHC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPE tiene 92356612 parametros <br>\n",
    "ProtTrans (prot_bert_bfd) tiene 419933186 (4x larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 92356612\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"params:\", pytorch_total_params)\n",
    "\n",
    "#from torchvision import models\n",
    "#from torchsummary import summary\n",
    "#summary(model, (60, 768))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTMHC CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "\n",
      "Cargamos los pesos de TAPE\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0\n",
      "tape.models.modeling_utils - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"base_model\": \"transformer\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 768,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_size\": 768,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 30\n",
      "}\n",
      "\n",
      "tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22\n",
      "tape.models.modeling_utils - Weights of BERTMHC_CNN not initialized from pretrained model: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
      "tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC_CNN: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTMHC_CNN(\n",
      "  (bert): ProteinBertModel(\n",
      "    (embeddings): ProteinBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(8192, 768)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ProteinBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ProteinBertLayer(\n",
      "          (attention): ProteinBertAttention(\n",
      "            (self): ProteinBertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ProteinBertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ProteinBertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ProteinBertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ProteinBertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (fc4): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "print(\"\\nCargamos los pesos de TAPE\\n\\n\")\n",
    "model = BERTMHC_CNN.from_pretrained('bert-base')\n",
    "\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3357 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 400]' is invalid for input of size 1161216",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_metrics \u001b[39m=\u001b[39m train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     eval_metrics \u001b[39m=\u001b[39m evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vicente/projects/neoantigen/BERTMHC/bertmhc/dataloader.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     eval_metrics[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_metrics\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/utils_model.py:296\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\u001b[0m\n\u001b[1;32m    293\u001b[0m instance_weights \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39minstance_weights\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    294\u001b[0m                              torch\u001b[39m.\u001b[39mones(batch[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], )\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m    295\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 296\u001b[0m logits, targets \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m    298\u001b[0m label_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m    299\u001b[0m mass_valid \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mtorch\u001b[39m.\u001b[39misnan(targets[:, \u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/neoantigen/BERTMHC/bertmhc/bertmhc.py:176\u001b[0m, in \u001b[0;36mBERTMHC_CNN.forward\u001b[0;34m(self, input_ids, input_mask, targets)\u001b[0m\n\u001b[1;32m    174\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))  \u001b[39m# [6, 14, 14] -> [16, 10, 10]\u001b[39;00m\n\u001b[1;32m    175\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)           \u001b[39m# [16, 10, 10] -> [16, 5, 5]\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m16\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m5\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m5\u001b[39;49m) \n\u001b[1;32m    177\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[1;32m    178\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 400]' is invalid for input of size 1161216"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.15\n",
    "w_pos = 1.0 # mass positive weight\n",
    "save = \"bertmhc_model.pt\"\n",
    "alpha = 0.0 # alpha weight on mass loss, affinity loss weight with 1-alpha\n",
    "patience = 5 # Earlystopping patience\n",
    "metric = 'val_auc' # validation metric, default auc\n",
    "\n",
    "aff_criterion = nn.BCEWithLogitsLoss()\n",
    "w_pos = torch.tensor([w_pos]).to(device)\n",
    "mass_criterion = nn.BCEWithLogitsLoss(pos_weight=w_pos, reduction='none')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, min_lr=1e-4, factor=0.1)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True, saveto=save)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Training epoch {}\".format(epoch))\n",
    "    train_metrics = train(model, optimizer, train_data, device, aff_criterion, mass_criterion, alpha, scheduler)\n",
    "    eval_metrics = evaluate(model, val_data, device, aff_criterion, mass_criterion, alpha)\n",
    "    eval_metrics['train_loss'] = train_metrics\n",
    "    logs = eval_metrics\n",
    "\n",
    "    scheduler.step(logs.get(metric))\n",
    "    logging.info('Sample dict log: %s' % logs)\n",
    "\n",
    "    # callbacks\n",
    "    early_stopping(-logs.get(metric), model, optimizer)\n",
    "    if early_stopping.early_stop or logs.get(metric) <= 0:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3573, -0.0828, -0.2150,  ...,  0.7775, -0.4792,  0.2593],\n",
      "         [-0.0204,  0.1773,  0.3248,  ..., -0.4780,  0.3139, -1.2075],\n",
      "         [ 0.5642, -0.1968, -0.3168,  ...,  0.6410, -0.5646, -0.0137],\n",
      "         ...,\n",
      "         [ 0.6202, -0.6668, -0.1588,  ..., -0.5109, -0.4736, -0.4340],\n",
      "         [ 0.9661, -0.4192, -0.0421,  ...,  0.3981, -0.4994,  0.6438],\n",
      "         [ 0.4085,  0.4986,  0.3141,  ..., -0.2522,  0.3626,  0.4391]],\n",
      "\n",
      "        [[ 0.3590, -0.8683, -0.0962,  ..., -0.6727, -0.3124, -0.3711],\n",
      "         [-0.3341, -0.4035,  0.3525,  ..., -0.2994, -1.4363, -0.5281],\n",
      "         [-0.0247, -0.2878, -0.5165,  ..., -0.4890,  0.0873,  0.9115],\n",
      "         ...,\n",
      "         [-0.0374, -0.4332,  0.2203,  ...,  0.3020, -0.4549,  0.3404],\n",
      "         [-0.6407, -0.8692, -0.5351,  ..., -0.3415, -0.5610,  0.5447],\n",
      "         [ 0.3778,  0.9431, -0.5460,  ..., -0.6170, -0.4355, -0.2854]],\n",
      "\n",
      "        [[-0.1983, -0.1177, -0.0210,  ...,  0.8559,  0.0914, -0.1541],\n",
      "         [-0.1341,  1.0969, -0.5761,  ...,  0.2242,  0.2491,  0.0366],\n",
      "         [-0.0506, -0.6308,  0.4706,  ..., -0.1516,  1.0757,  0.8622],\n",
      "         ...,\n",
      "         [ 0.6311, -0.8034,  0.2837,  ...,  0.2363, -1.1728,  0.3148],\n",
      "         [ 0.2033,  1.0351, -0.2812,  ..., -0.4588, -0.6004,  0.7430],\n",
      "         [-0.7162,  0.2104, -0.0124,  ...,  0.6214, -0.1764,  1.0048]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1216,  0.3253,  0.1260,  ..., -0.0026, -0.3905,  0.1734],\n",
      "         [ 1.2661, -0.1670,  0.8898,  ...,  0.8612, -0.2389, -0.2712],\n",
      "         [ 0.1925, -0.7204,  0.2071,  ...,  0.0067, -0.0515, -0.5239],\n",
      "         ...,\n",
      "         [ 0.8751,  0.4645,  0.3054,  ...,  0.0492, -1.0970,  0.4197],\n",
      "         [ 0.6873,  0.4798, -0.6554,  ..., -0.2036, -0.7676,  0.2309],\n",
      "         [-0.2343,  0.6811,  0.3823,  ...,  1.3969,  0.2903, -0.2928]],\n",
      "\n",
      "        [[ 0.1293, -0.6108,  0.3026,  ..., -0.2223, -1.7490,  0.2955],\n",
      "         [-0.0431, -0.5992, -0.7256,  ...,  0.1773,  0.1743, -0.0106],\n",
      "         [-0.1308,  0.2666,  0.3378,  ..., -0.6370,  0.8238,  0.8658],\n",
      "         ...,\n",
      "         [ 0.3462, -0.4593,  0.2402,  ..., -0.2554, -0.0333, -0.0688],\n",
      "         [-0.4334,  0.3717,  0.6498,  ...,  0.3909, -0.6612,  0.0073],\n",
      "         [-1.2931, -0.6359,  0.0769,  ...,  0.5763,  0.3958,  0.2175]],\n",
      "\n",
      "        [[ 0.2070, -0.0954, -0.2115,  ..., -0.3568,  0.0089,  0.8550],\n",
      "         [ 0.3316, -0.4941, -0.0199,  ...,  0.3102, -0.1123, -0.7573],\n",
      "         [ 0.0530,  0.0440, -0.0615,  ..., -0.4814, -0.8141,  0.1566],\n",
      "         ...,\n",
      "         [ 0.9448,  0.2198, -0.3728,  ..., -0.8632, -1.1478, -0.8851],\n",
      "         [ 0.9156, -0.6033, -0.8065,  ...,  0.7807, -0.1252, -1.0052],\n",
      "         [-0.5463, -0.5445, -0.5537,  ...,  0.2935, -0.0109, -0.1342]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[-3.7914e-01,  4.4920e-01, -1.4163e-01,  ..., -4.1250e-01,\n",
      "            2.6966e-01, -1.0879e+00],\n",
      "          [-1.6120e-01,  7.4956e-01,  1.4806e+00,  ..., -3.9624e-01,\n",
      "           -6.0684e-02,  1.2057e+00],\n",
      "          [-2.0727e-01, -6.9496e-01, -5.2973e-01,  ..., -1.9744e-02,\n",
      "           -4.3601e-01, -7.2663e-01],\n",
      "          ...,\n",
      "          [-3.5567e-02, -1.0302e+00, -6.8258e-01,  ..., -1.6902e+00,\n",
      "           -1.2133e+00,  1.5823e+00],\n",
      "          [ 3.0272e-01,  7.0002e-01,  4.9343e-01,  ...,  9.0565e-01,\n",
      "            8.0288e-01, -6.7458e-01],\n",
      "          [-6.1200e-01, -9.0104e-01, -2.4282e-01,  ..., -6.4110e-01,\n",
      "            2.7555e-02,  1.0039e+00]],\n",
      "\n",
      "         [[-1.6486e-01,  2.5925e-02, -9.6700e-03,  ..., -4.0683e-02,\n",
      "           -8.5645e-01,  6.1376e-01],\n",
      "          [ 3.7098e-01, -7.5825e-01,  4.7282e-02,  ...,  1.8472e-01,\n",
      "            1.3445e-01, -5.5352e-01],\n",
      "          [-5.8501e-01,  1.3633e+00, -2.6733e-01,  ..., -1.7119e-01,\n",
      "            7.1314e-01, -7.2373e-01],\n",
      "          ...,\n",
      "          [-6.5270e-01, -1.2521e+00,  5.5101e-01,  ..., -8.1036e-01,\n",
      "            3.0056e-01, -4.6568e-01],\n",
      "          [-4.1359e-02,  8.4621e-02, -2.3545e-01,  ..., -3.1174e-01,\n",
      "            5.1304e-01,  3.0802e-01],\n",
      "          [ 6.2626e-01, -1.1047e+00,  3.5211e-01,  ..., -1.3762e-01,\n",
      "           -7.1801e-01,  4.3615e-01]],\n",
      "\n",
      "         [[-7.9217e-01,  1.1041e+00,  1.1835e+00,  ..., -8.9700e-02,\n",
      "            4.5234e-01,  4.4512e-01],\n",
      "          [-2.2666e-02, -3.9566e-01,  1.1247e+00,  ..., -2.1616e-01,\n",
      "           -7.9387e-01,  3.9590e-01],\n",
      "          [-1.8606e-02,  9.9868e-02, -8.3504e-01,  ..., -5.6358e-01,\n",
      "            3.9373e-01, -4.6574e-01],\n",
      "          ...,\n",
      "          [ 1.7347e+00,  1.3148e+00,  7.4979e-01,  ...,  4.2123e-01,\n",
      "           -1.1165e+00, -2.5219e-01],\n",
      "          [-1.2090e+00, -4.4594e-01,  1.7066e-01,  ..., -9.0101e-01,\n",
      "            9.9139e-01,  8.0188e-01],\n",
      "          [ 5.5598e-01, -1.5766e-01, -9.4538e-01,  ...,  1.0200e+00,\n",
      "           -3.2234e-01,  1.3169e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.1567e-01, -7.7939e-01, -1.7371e+00,  ...,  3.1072e-01,\n",
      "           -2.0424e-01, -1.4276e-01],\n",
      "          [ 3.0701e-01, -8.6557e-02, -7.7006e-01,  ..., -3.3193e-01,\n",
      "            5.4121e-01, -3.1045e-01],\n",
      "          [ 7.1873e-02, -4.7184e-01,  3.3895e-01,  ...,  9.1706e-01,\n",
      "           -2.6029e-02,  5.2699e-01],\n",
      "          ...,\n",
      "          [-3.2360e-02, -7.5444e-01, -1.0996e+00,  ...,  8.0338e-02,\n",
      "            7.3688e-01, -8.3803e-01],\n",
      "          [ 4.9471e-01,  1.3321e-01, -1.0012e-01,  ...,  6.1250e-01,\n",
      "           -9.4309e-01, -1.1306e+00],\n",
      "          [-3.1270e-01,  7.3502e-01,  1.0047e+00,  ..., -2.3170e-01,\n",
      "           -3.3013e-01, -5.9335e-02]],\n",
      "\n",
      "         [[-1.3929e-01,  6.4836e-01, -6.5261e-01,  ...,  5.6540e-01,\n",
      "           -1.4971e-01,  6.8547e-01],\n",
      "          [ 4.5885e-01, -3.0687e-01, -4.6730e-01,  ..., -5.1286e-01,\n",
      "           -1.9459e-01, -4.8472e-01],\n",
      "          [-1.3319e-02,  6.1427e-01, -2.4452e-01,  ..., -1.1285e-01,\n",
      "            1.0338e+00,  8.9347e-02],\n",
      "          ...,\n",
      "          [ 6.8616e-01,  5.3697e-01,  2.2086e-01,  ...,  9.8364e-01,\n",
      "            8.7737e-01, -1.0806e+00],\n",
      "          [-6.7805e-01, -1.0930e+00, -4.4748e-01,  ..., -1.0729e+00,\n",
      "            3.9565e-01, -1.2644e-01],\n",
      "          [ 8.5400e-01, -6.9061e-02,  7.2511e-01,  ...,  8.6613e-01,\n",
      "           -5.0854e-01, -3.2157e-01]],\n",
      "\n",
      "         [[-6.5709e-01,  8.3377e-01, -2.7997e-01,  ...,  4.3773e-01,\n",
      "            3.1062e-01, -3.2560e-01],\n",
      "          [-3.4205e-01,  2.6695e-02,  8.4644e-01,  ..., -3.9016e-01,\n",
      "            1.8550e-01,  2.2644e-01],\n",
      "          [-2.2066e-01,  7.9034e-01,  4.6239e-01,  ...,  1.8177e-01,\n",
      "            1.6540e+00, -3.6998e-01],\n",
      "          ...,\n",
      "          [ 5.2139e-01, -1.8332e-01,  4.5886e-01,  ..., -1.8961e-01,\n",
      "           -4.2548e-01,  2.0691e-02],\n",
      "          [ 2.2992e-01, -8.2170e-01,  2.7093e-01,  ..., -1.5001e+00,\n",
      "            4.7037e-01,  7.9119e-01],\n",
      "          [ 2.3301e+00, -5.1258e-01,  7.7769e-01,  ...,  3.1241e-01,\n",
      "           -3.7894e-01,  1.0644e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6638e-01, -1.5386e-01,  5.4967e-01,  ...,  1.3944e-01,\n",
      "           -5.4355e-01, -8.9082e-01],\n",
      "          [ 7.8204e-01,  3.3023e-01,  2.3761e-01,  ...,  4.7299e-02,\n",
      "            1.6450e-01, -6.0559e-01],\n",
      "          [-6.4117e-01, -6.4671e-01,  8.9608e-01,  ..., -4.5596e-02,\n",
      "           -5.7721e-02,  7.9665e-01],\n",
      "          ...,\n",
      "          [-5.1097e-01,  8.9544e-01,  1.0023e+00,  ..., -2.6790e-01,\n",
      "           -7.0560e-01, -7.0891e-02],\n",
      "          [-2.7217e-02, -7.1140e-01, -2.5338e-01,  ...,  2.2595e-01,\n",
      "           -5.5416e-01,  6.0532e-01],\n",
      "          [-4.6216e-01,  1.1126e+00, -4.3870e-01,  ..., -1.2996e+00,\n",
      "           -6.9856e-02, -7.7826e-01]],\n",
      "\n",
      "         [[-6.7405e-01,  4.1399e-01, -6.9491e-01,  ..., -9.2040e-01,\n",
      "            6.5213e-01, -2.3890e-01],\n",
      "          [-8.9827e-01,  7.9263e-01, -8.2165e-01,  ..., -5.4047e-01,\n",
      "           -1.3437e-01, -1.9739e-01],\n",
      "          [ 2.1809e-01,  5.3665e-01, -3.8525e-01,  ..., -1.2147e-01,\n",
      "           -1.4279e-02, -9.2100e-01],\n",
      "          ...,\n",
      "          [-3.4983e-01, -1.1122e-01, -3.9900e-01,  ..., -9.7112e-02,\n",
      "           -1.3269e+00,  7.1369e-01],\n",
      "          [ 4.3183e-01, -8.4303e-01,  1.0750e-02,  ..., -6.5679e-01,\n",
      "            7.4688e-01, -1.0497e+00],\n",
      "          [-6.2395e-01,  9.9306e-01, -5.0199e-01,  ..., -1.8868e-01,\n",
      "           -3.4029e-01,  5.9106e-01]],\n",
      "\n",
      "         [[ 5.7614e-01, -9.9529e-01,  4.5361e-01,  ...,  4.1336e-01,\n",
      "           -6.9498e-02, -2.3916e-01],\n",
      "          [ 3.4735e-01, -1.3669e-01,  1.3759e+00,  ...,  7.0274e-02,\n",
      "           -9.2795e-02,  8.9541e-01],\n",
      "          [ 5.5103e-01, -5.1495e-01, -2.2632e-01,  ..., -5.5700e-02,\n",
      "           -8.0088e-01, -4.4887e-01],\n",
      "          ...,\n",
      "          [-8.0117e-01, -2.7145e-01,  8.9002e-01,  ...,  9.9217e-01,\n",
      "           -1.4737e+00,  7.7637e-01],\n",
      "          [ 3.5904e-01,  7.7883e-01, -2.5220e-01,  ..., -7.5487e-01,\n",
      "           -2.6357e-01, -1.2350e-01],\n",
      "          [-1.5356e+00,  4.1812e-01,  2.8569e-01,  ..., -2.3712e-01,\n",
      "            5.0119e-01, -7.2501e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.3748e-02,  1.1608e-01,  1.7917e-01,  ...,  2.5480e-01,\n",
      "           -4.6684e-01,  4.9006e-01],\n",
      "          [ 2.7936e-02, -5.8372e-02, -9.3421e-01,  ...,  2.8430e-02,\n",
      "           -4.2030e-02, -2.3194e-01],\n",
      "          [-1.3411e-01, -2.4339e-01,  2.7051e-01,  ..., -2.1073e-01,\n",
      "            8.2325e-01,  8.2660e-01],\n",
      "          ...,\n",
      "          [ 4.0686e-01,  4.8775e-01, -7.7870e-01,  ..., -3.4350e-02,\n",
      "            1.6069e+00, -2.5118e-01],\n",
      "          [ 1.9356e-01, -3.1579e-01, -1.6852e-01,  ...,  5.8338e-01,\n",
      "            8.6013e-01, -8.3710e-01],\n",
      "          [ 6.8315e-01, -7.3162e-01, -1.8643e-01,  ...,  7.9740e-01,\n",
      "           -5.3947e-01,  3.4839e-01]],\n",
      "\n",
      "         [[ 7.3717e-02, -4.2908e-01,  3.6660e-02,  ...,  4.3287e-01,\n",
      "           -3.4303e-01,  8.4301e-02],\n",
      "          [-1.2875e-01,  3.5517e-01, -1.7209e-02,  ...,  1.5985e-01,\n",
      "           -6.2113e-01, -2.2227e-02],\n",
      "          [ 7.0218e-01,  2.0669e-02, -4.2439e-01,  ..., -3.0740e-01,\n",
      "            8.7616e-03, -3.4498e-01],\n",
      "          ...,\n",
      "          [-8.3062e-03, -2.6760e-01, -2.7485e-01,  ...,  4.2844e-01,\n",
      "           -2.6013e-01,  1.1987e+00],\n",
      "          [ 9.6898e-01,  4.9270e-01, -1.9591e-01,  ..., -5.4496e-01,\n",
      "            8.6627e-01, -9.3524e-01],\n",
      "          [-5.4207e-01, -2.0790e-01, -6.4564e-01,  ...,  5.0301e-01,\n",
      "            1.4391e-01, -3.7687e-01]],\n",
      "\n",
      "         [[-7.7144e-01,  3.5951e-01, -7.4510e-01,  ...,  3.7672e-01,\n",
      "            3.1070e-01,  1.1176e-01],\n",
      "          [-3.8146e-01,  1.0244e+00, -5.8502e-02,  ...,  1.7918e-01,\n",
      "            3.9215e-01,  4.8279e-01],\n",
      "          [ 5.7110e-01,  7.7225e-01,  2.2698e-01,  ..., -2.1957e-01,\n",
      "            3.2703e-01,  4.3537e-01],\n",
      "          ...,\n",
      "          [ 8.9048e-01, -7.9894e-02,  2.6932e-01,  ...,  9.3772e-01,\n",
      "           -7.5204e-01,  3.7253e-01],\n",
      "          [ 1.1677e+00, -4.5717e-02,  7.7776e-02,  ...,  6.0468e-01,\n",
      "           -2.6823e-01,  1.0780e-01],\n",
      "          [-1.0409e+00,  1.2570e+00, -5.9994e-01,  ...,  2.7343e-01,\n",
      "            7.5970e-01, -5.6171e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.3068e-01, -1.0632e-01, -1.6518e-01,  ...,  1.6586e-01,\n",
      "           -4.0531e-01, -6.9008e-01],\n",
      "          [ 6.9910e-02,  3.9545e-01, -7.2522e-01,  ...,  3.4307e-01,\n",
      "            2.7341e-01,  4.6856e-01],\n",
      "          [-8.0530e-01,  1.1822e-01,  7.7316e-01,  ..., -7.2232e-01,\n",
      "            1.0888e+00, -2.9690e-01],\n",
      "          ...,\n",
      "          [-1.8651e-01, -5.6721e-01,  7.0292e-01,  ...,  3.9941e-01,\n",
      "            8.7611e-01, -3.7010e-02],\n",
      "          [ 7.1523e-01,  7.2629e-02, -7.0393e-02,  ..., -1.3498e-01,\n",
      "            3.4609e-02,  1.2076e+00],\n",
      "          [-1.1304e+00, -4.0777e-01, -7.7125e-01,  ...,  4.4212e-01,\n",
      "            8.0070e-01, -5.0318e-01]],\n",
      "\n",
      "         [[ 9.9624e-01, -5.6310e-01,  5.9763e-01,  ..., -1.9143e-01,\n",
      "            2.7938e-01,  1.7390e-01],\n",
      "          [-1.2122e-01, -3.8715e-01, -7.4000e-01,  ...,  1.1811e+00,\n",
      "           -1.2182e+00, -2.0392e-01],\n",
      "          [ 8.6967e-01, -1.1684e+00,  2.2808e-01,  ...,  3.8595e-01,\n",
      "           -4.6011e-01,  7.1690e-01],\n",
      "          ...,\n",
      "          [ 3.6049e-01,  6.8665e-05, -1.9034e-01,  ..., -1.5672e-01,\n",
      "           -1.1517e+00,  6.6736e-01],\n",
      "          [-4.8345e-01,  3.2720e-01, -1.1267e+00,  ...,  7.3732e-01,\n",
      "            5.6553e-01, -4.5018e-01],\n",
      "          [ 1.8120e-01,  1.5710e-01,  4.8509e-01,  ..., -4.5203e-01,\n",
      "           -1.2839e+00,  4.3993e-01]],\n",
      "\n",
      "         [[-1.6274e-01,  1.6992e-01, -2.8609e-01,  ..., -2.3862e-03,\n",
      "            7.1917e-01,  3.8595e-01],\n",
      "          [-1.2109e+00,  1.1055e+00, -1.2019e-02,  ..., -7.5290e-02,\n",
      "            1.8264e+00,  3.1494e-02],\n",
      "          [-5.8948e-01, -3.6646e-01, -3.9630e-01,  ..., -6.4350e-01,\n",
      "           -3.7275e-01,  2.0318e-01],\n",
      "          ...,\n",
      "          [ 1.4455e-01, -1.3500e+00, -1.0742e-01,  ...,  2.6639e-02,\n",
      "            3.6672e-01,  1.6754e+00],\n",
      "          [-1.6731e-01,  2.8967e-01, -3.0977e-01,  ..., -8.4825e-01,\n",
      "            2.2904e-01, -6.1485e-01],\n",
      "          [-3.3767e-01, -1.5359e-02, -3.3369e-01,  ...,  1.2027e+00,\n",
      "            1.6651e+00,  2.7174e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3888e+00,  4.0233e-01,  2.1369e-01,  ..., -4.5416e-01,\n",
      "            1.8978e-01, -1.2770e+00],\n",
      "          [ 8.8248e-01,  6.0490e-01, -1.1654e-01,  ..., -2.0870e-01,\n",
      "           -1.4688e+00, -4.0867e-01],\n",
      "          [ 1.3251e+00,  4.4770e-02,  1.6876e-01,  ..., -6.1803e-01,\n",
      "            6.3565e-01,  4.1360e-01],\n",
      "          ...,\n",
      "          [-2.9043e-01,  4.2020e-01,  4.0080e-01,  ..., -2.2365e-01,\n",
      "            2.1222e-01, -1.5740e+00],\n",
      "          [-1.3776e-01,  3.7628e-01,  7.8553e-01,  ..., -5.9251e-02,\n",
      "           -2.9738e-01, -5.0486e-01],\n",
      "          [ 2.2165e-01, -1.4608e-01,  2.5682e-02,  ..., -5.1758e-01,\n",
      "           -1.1033e+00,  1.2258e-01]],\n",
      "\n",
      "         [[-9.6664e-01, -6.3063e-01,  6.0674e-01,  ..., -1.3568e-01,\n",
      "            9.2850e-01,  1.7970e-01],\n",
      "          [-8.4933e-01,  1.3514e+00,  1.4985e-02,  ...,  6.3534e-01,\n",
      "           -5.7586e-01, -1.1221e+00],\n",
      "          [ 1.5019e+00,  1.1765e-01, -5.4371e-01,  ..., -4.3340e-01,\n",
      "           -7.0541e-01,  8.8588e-01],\n",
      "          ...,\n",
      "          [ 2.5607e-01, -9.0754e-01, -3.1385e-01,  ...,  1.6993e-02,\n",
      "           -1.2555e-01,  5.6040e-01],\n",
      "          [-3.9507e-01,  3.9444e-01,  5.5133e-01,  ..., -3.1949e-01,\n",
      "            4.0644e-01, -1.4781e+00],\n",
      "          [ 1.9106e-01,  1.9767e-01,  8.1131e-02,  ...,  5.5051e-01,\n",
      "            1.8793e-02,  6.9756e-01]],\n",
      "\n",
      "         [[ 9.3903e-01, -6.4648e-01,  1.4006e+00,  ...,  1.1126e+00,\n",
      "            7.1433e-04,  5.9625e-01],\n",
      "          [ 2.9748e-01,  1.2720e+00, -1.2127e-01,  ...,  1.4952e+00,\n",
      "           -2.9850e-01,  5.2003e-02],\n",
      "          [ 6.4689e-01, -1.1440e-02, -7.7537e-01,  ...,  6.3289e-01,\n",
      "            1.7355e-01,  1.2454e+00],\n",
      "          ...,\n",
      "          [ 3.2686e-02, -4.4274e-01,  2.6178e-01,  ...,  2.2426e-01,\n",
      "           -7.5670e-01,  1.0132e+00],\n",
      "          [ 5.7612e-01,  4.3703e-01,  1.7114e-01,  ...,  1.8172e-01,\n",
      "            9.6802e-02,  4.2695e-01],\n",
      "          [ 4.9595e-02,  8.3681e-01, -5.0889e-01,  ...,  4.3710e-01,\n",
      "            1.1963e-01,  7.3976e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.3454e-01, -7.8626e-01,  1.7002e-01,  ...,  4.7446e-01,\n",
      "           -1.2871e-01, -4.9777e-01],\n",
      "          [-5.4783e-01, -6.2847e-01, -1.3090e+00,  ..., -2.1653e-01,\n",
      "            2.3100e-01, -1.0019e-01],\n",
      "          [-1.2230e-01, -2.8179e-01, -9.6127e-01,  ...,  7.4545e-02,\n",
      "            2.5099e-01, -1.2058e-01],\n",
      "          ...,\n",
      "          [-3.5074e-02,  5.6617e-01,  1.0116e+00,  ...,  5.2692e-01,\n",
      "            9.1543e-01,  1.8829e-01],\n",
      "          [ 1.4758e-01, -6.4555e-01,  3.8505e-01,  ...,  5.8599e-02,\n",
      "            9.3411e-02, -5.3774e-01],\n",
      "          [-8.9190e-02,  1.0800e+00, -7.8630e-01,  ...,  9.8480e-02,\n",
      "            1.2180e-02, -7.3482e-03]],\n",
      "\n",
      "         [[ 6.2240e-01, -2.6896e-01, -4.9013e-01,  ..., -3.9190e-01,\n",
      "            1.5487e-02, -2.2099e-01],\n",
      "          [ 1.0526e-01, -4.6082e-01,  3.5224e-01,  ...,  4.5883e-01,\n",
      "           -1.3908e+00,  1.4395e+00],\n",
      "          [-9.9856e-01, -4.7286e-01, -4.7693e-01,  ..., -3.7304e-01,\n",
      "            7.3105e-01, -6.7742e-01],\n",
      "          ...,\n",
      "          [-9.2972e-02, -3.7579e-01,  9.5221e-02,  ...,  3.4424e-02,\n",
      "           -1.4603e-01, -5.9745e-01],\n",
      "          [ 1.8987e-01,  2.3472e-01, -4.1032e-01,  ..., -8.8799e-02,\n",
      "            8.9536e-01, -1.1524e+00],\n",
      "          [-6.0931e-02, -6.4407e-01,  9.8542e-01,  ...,  5.0175e-01,\n",
      "           -2.2646e-01, -2.9067e-01]],\n",
      "\n",
      "         [[-3.6714e-01, -4.2951e-01, -1.3701e+00,  ..., -3.3381e-01,\n",
      "            7.0658e-01, -5.6014e-01],\n",
      "          [-2.9897e-02, -4.1910e-01, -1.1516e+00,  ...,  4.6672e-01,\n",
      "            1.4278e-01,  1.6872e-01],\n",
      "          [-3.4567e-01,  1.3986e-01,  3.5684e-01,  ..., -7.7633e-01,\n",
      "            6.6653e-01, -7.3957e-01],\n",
      "          ...,\n",
      "          [-1.0649e-02, -3.6888e-01,  8.4385e-01,  ..., -2.2464e-01,\n",
      "            4.9355e-01,  5.3369e-01],\n",
      "          [ 5.4821e-01,  1.1571e+00, -1.1155e+00,  ..., -4.9819e-01,\n",
      "            4.6684e-02, -5.1346e-01],\n",
      "          [-1.8225e-03,  5.6456e-01,  1.2227e+00,  ...,  2.3687e-01,\n",
      "           -2.6024e-01,  1.1696e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2446e-01,  6.2850e-01,  1.0035e+00,  ...,  1.8593e-01,\n",
      "            1.6845e-01, -4.2180e-02],\n",
      "          [ 4.1823e-01,  8.8082e-01,  1.6022e+00,  ..., -1.6539e-01,\n",
      "           -1.1686e-01, -6.4529e-01],\n",
      "          [ 1.0489e+00,  5.1302e-01, -3.2933e-01,  ...,  3.3705e-01,\n",
      "           -2.7085e-01,  2.7977e-01],\n",
      "          ...,\n",
      "          [ 4.0318e-02,  4.8960e-01, -1.0076e+00,  ..., -1.6700e-01,\n",
      "           -9.0636e-01,  3.7192e-01],\n",
      "          [-4.5355e-01, -1.2055e+00,  3.3189e-01,  ...,  2.1647e-01,\n",
      "           -2.7942e-01,  1.2670e+00],\n",
      "          [-4.1859e-01, -3.9153e-01, -4.7209e-01,  ...,  3.9951e-02,\n",
      "           -1.7164e-01, -1.9431e-01]],\n",
      "\n",
      "         [[-3.1724e-01,  4.7645e-02, -1.0031e+00,  ..., -8.1538e-01,\n",
      "            8.7440e-01, -7.1141e-02],\n",
      "          [ 1.4491e-01,  4.5362e-02,  9.6893e-01,  ...,  4.7633e-01,\n",
      "           -4.9831e-01,  1.0010e-02],\n",
      "          [ 4.2087e-01,  4.7774e-01, -1.6218e-01,  ..., -5.2990e-01,\n",
      "            4.4466e-01, -6.9204e-01],\n",
      "          ...,\n",
      "          [ 2.5820e-01,  2.9114e-01, -8.4334e-03,  ..., -5.6383e-02,\n",
      "           -1.0436e+00, -3.3055e-01],\n",
      "          [ 3.6085e-01,  3.1323e-01, -5.5233e-01,  ...,  1.6018e-01,\n",
      "            1.6380e-01,  3.2650e-02],\n",
      "          [-3.1362e-01, -7.9589e-01,  1.3351e+00,  ...,  1.0743e+00,\n",
      "           -2.0594e-01,  4.1935e-01]],\n",
      "\n",
      "         [[ 5.6509e-01,  4.0950e-01,  7.0203e-03,  ..., -1.3559e-01,\n",
      "            6.0379e-02,  6.1622e-01],\n",
      "          [ 1.3757e+00,  5.0838e-01,  5.6573e-01,  ...,  8.9000e-01,\n",
      "           -1.0832e+00,  1.2865e+00],\n",
      "          [ 2.8450e-01,  2.6446e-01, -3.8432e-01,  ...,  3.6936e-01,\n",
      "            6.5562e-01,  3.7674e-01],\n",
      "          ...,\n",
      "          [ 3.5914e-01, -2.7016e-01,  6.6753e-01,  ..., -2.1155e-01,\n",
      "            4.8031e-01,  3.3150e-01],\n",
      "          [ 6.7442e-01, -4.1905e-02, -2.1532e-01,  ..., -1.0662e-01,\n",
      "            1.8626e+00, -8.9349e-01],\n",
      "          [ 3.9538e-01,  9.5964e-02,  7.1837e-01,  ...,  4.9081e-01,\n",
      "            1.0377e+00, -1.5702e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.7691e-01,  4.3088e-01,  6.8928e-02,  ..., -3.1197e-01,\n",
      "            4.5870e-01, -6.9392e-01],\n",
      "          [-1.3008e+00, -4.8991e-01, -4.0946e-01,  ...,  2.7256e-01,\n",
      "           -1.0595e-01, -8.5034e-01],\n",
      "          [ 3.5017e-01, -9.3578e-02, -5.9869e-01,  ...,  3.8846e-01,\n",
      "            6.3491e-01,  3.3731e-01],\n",
      "          ...,\n",
      "          [ 1.8310e-01,  6.4612e-01, -5.3668e-01,  ..., -6.0669e-01,\n",
      "           -4.9965e-01, -9.2559e-01],\n",
      "          [ 5.8234e-02,  2.2915e-01, -4.7546e-01,  ...,  1.0436e+00,\n",
      "            3.0645e-01,  8.4310e-01],\n",
      "          [ 2.4832e-01,  8.7936e-02,  6.4670e-01,  ..., -5.5431e-01,\n",
      "           -4.5487e-01,  5.4199e-01]],\n",
      "\n",
      "         [[ 1.2988e+00, -1.0930e+00,  1.3386e-01,  ..., -4.7538e-01,\n",
      "           -1.0235e+00,  4.5558e-01],\n",
      "          [-5.2384e-01,  1.8569e-01,  2.3027e-01,  ...,  5.9218e-01,\n",
      "           -2.9708e-01, -3.5035e-01],\n",
      "          [ 2.1571e-02, -2.1144e-01,  3.7541e-01,  ...,  7.6856e-01,\n",
      "           -1.3958e+00, -3.6613e-01],\n",
      "          ...,\n",
      "          [-1.3562e+00,  6.1562e-01, -9.9464e-01,  ...,  7.5930e-01,\n",
      "           -6.1648e-01,  7.0731e-01],\n",
      "          [ 5.0946e-01, -3.7322e-01, -1.6054e-01,  ..., -5.8004e-01,\n",
      "           -3.2422e-01, -1.3943e+00],\n",
      "          [ 8.0899e-02,  3.7541e-01, -8.7027e-01,  ...,  7.6836e-01,\n",
      "            1.9884e-01,  9.6231e-02]],\n",
      "\n",
      "         [[-5.1469e-01,  7.3377e-01,  6.1147e-02,  ...,  3.7331e-01,\n",
      "            1.1249e+00, -1.2402e-01],\n",
      "          [ 3.8489e-01, -5.4645e-01, -8.9925e-01,  ..., -4.4820e-01,\n",
      "            6.5248e-01, -5.5704e-01],\n",
      "          [-3.3195e-01, -3.9205e-01,  3.3731e-01,  ..., -1.4181e-01,\n",
      "           -2.1870e-01,  6.5676e-01],\n",
      "          ...,\n",
      "          [-5.2770e-01,  7.5485e-01, -1.1257e-01,  ..., -1.6188e-01,\n",
      "           -8.1416e-01, -2.3358e-01],\n",
      "          [-7.0010e-02,  2.5613e-01,  4.9777e-01,  ...,  5.2164e-02,\n",
      "           -1.5898e-01,  4.8563e-01],\n",
      "          [ 2.6685e-01, -6.7092e-01,  1.0085e+00,  ..., -2.4270e-02,\n",
      "           -2.9392e-01, -2.2651e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.9832e-01, -4.4147e-01,  3.8636e-02,  ..., -3.0120e-01,\n",
      "           -7.3931e-01,  2.2701e-01],\n",
      "          [-5.0387e-01,  2.2761e-01,  1.0162e+00,  ..., -3.4419e-01,\n",
      "           -3.1561e-01,  7.4459e-01],\n",
      "          [ 6.7897e-02,  9.4351e-01, -1.7550e-01,  ..., -1.1364e-01,\n",
      "            9.6677e-01,  3.2793e-01],\n",
      "          ...,\n",
      "          [ 2.8138e-01, -5.5087e-01,  4.0290e-01,  ...,  3.1188e-01,\n",
      "            3.9118e-01,  6.0542e-01],\n",
      "          [ 2.9005e-01, -6.7692e-01,  1.0643e-01,  ...,  8.5127e-02,\n",
      "            6.4456e-01,  8.9439e-02],\n",
      "          [-3.7477e-01,  1.8648e-01, -1.1379e-01,  ..., -8.0950e-01,\n",
      "           -1.1888e-01,  1.6917e+00]],\n",
      "\n",
      "         [[ 1.1969e+00,  1.0606e-01, -7.1889e-01,  ...,  3.9704e-01,\n",
      "           -8.5621e-01,  2.2763e-01],\n",
      "          [ 3.3773e-01, -1.6725e-01,  1.5210e-01,  ..., -5.7860e-01,\n",
      "           -6.2337e-01, -1.5844e-01],\n",
      "          [-1.1221e+00,  5.0084e-01,  6.6717e-01,  ...,  3.4544e-01,\n",
      "           -3.7792e-01,  6.2347e-01],\n",
      "          ...,\n",
      "          [-3.9223e-01, -1.9890e-01, -1.5009e-01,  ...,  3.9159e-01,\n",
      "           -4.5746e-01,  1.9761e-01],\n",
      "          [ 4.9308e-01, -4.1996e-01, -1.9507e-01,  ...,  1.4505e-01,\n",
      "            1.4975e-01, -1.7394e-01],\n",
      "          [ 3.7218e-01, -4.2778e-01, -1.3203e-01,  ...,  1.7841e-01,\n",
      "           -2.5168e-01,  1.5758e-01]],\n",
      "\n",
      "         [[ 1.6037e+00,  9.8723e-01,  5.6698e-01,  ...,  1.2769e+00,\n",
      "           -6.1756e-01,  4.2964e-01],\n",
      "          [-1.3622e-02,  3.3249e-01,  9.9416e-01,  ...,  5.7144e-01,\n",
      "            3.8825e-01, -5.1770e-01],\n",
      "          [-7.2865e-01,  2.1786e-01,  1.6158e+00,  ...,  1.6420e+00,\n",
      "            1.0756e-01,  2.3060e-01],\n",
      "          ...,\n",
      "          [-1.6838e-01,  5.3029e-01, -5.9200e-01,  ...,  7.6742e-02,\n",
      "            4.9994e-01,  2.1892e-01],\n",
      "          [-1.1013e-01,  1.2024e+00, -9.9710e-01,  ...,  4.4993e-01,\n",
      "           -1.8056e-01,  4.7797e-01],\n",
      "          [ 4.7957e-01,  9.0174e-01,  2.6593e-01,  ...,  7.0294e-01,\n",
      "           -5.6945e-01, -3.1450e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.7157e-02,  7.5956e-02, -8.6778e-02,  ...,  1.0875e+00,\n",
      "            9.4153e-02, -4.9115e-02],\n",
      "          [ 4.6846e-02, -6.3803e-01, -9.2498e-01,  ...,  3.5868e-01,\n",
      "            7.5243e-01,  3.9303e-02],\n",
      "          [-3.1999e-01,  3.7025e-01,  2.2780e-01,  ...,  1.0804e-01,\n",
      "            1.2670e+00,  6.3902e-01],\n",
      "          ...,\n",
      "          [-4.2477e-01, -1.1064e-02,  2.0027e-01,  ...,  2.7808e-01,\n",
      "           -7.6899e-01, -9.3591e-01],\n",
      "          [ 6.0245e-01,  4.5167e-01,  1.7346e-02,  ..., -5.0362e-01,\n",
      "           -2.4323e-01,  3.2006e-01],\n",
      "          [-2.5570e-01,  2.6008e-01, -1.0814e-01,  ...,  1.5587e-01,\n",
      "           -2.8466e-01, -9.1118e-01]],\n",
      "\n",
      "         [[-2.2961e-01,  1.0807e-01,  4.0596e-01,  ...,  2.7361e-01,\n",
      "           -3.2096e-01,  1.5547e-01],\n",
      "          [-6.4603e-01,  5.4378e-01, -2.6295e-01,  ..., -7.9425e-01,\n",
      "            7.7011e-02, -6.7166e-01],\n",
      "          [-3.7980e-01, -5.3596e-01, -2.6367e-01,  ...,  1.6749e-01,\n",
      "           -5.7830e-01,  5.6312e-01],\n",
      "          ...,\n",
      "          [ 4.2454e-01,  2.7930e-01,  5.2502e-01,  ..., -3.8531e-01,\n",
      "            5.1807e-01, -8.9507e-01],\n",
      "          [-3.8291e-01,  1.2610e-01, -8.3935e-01,  ...,  7.0619e-01,\n",
      "           -7.3947e-01,  9.2550e-02],\n",
      "          [ 7.8363e-01, -3.8464e-01,  7.7383e-01,  ..., -4.3393e-01,\n",
      "            4.7440e-01, -8.4388e-01]],\n",
      "\n",
      "         [[ 4.2186e-01, -7.1702e-01,  1.0575e-01,  ...,  3.4129e-02,\n",
      "            8.6896e-02,  3.6010e-01],\n",
      "          [ 6.4627e-03,  5.6625e-01, -3.1206e-01,  ..., -2.1566e-01,\n",
      "            1.4045e+00,  2.6411e-01],\n",
      "          [ 8.0802e-03,  1.6368e-02, -5.3325e-01,  ...,  6.8692e-01,\n",
      "            3.3954e-01,  9.7566e-01],\n",
      "          ...,\n",
      "          [ 6.6454e-02, -2.4958e-01, -7.7098e-02,  ..., -1.8355e-01,\n",
      "            8.0718e-01,  2.7111e-01],\n",
      "          [ 6.1352e-02,  1.2564e+00,  2.7907e-01,  ...,  7.4266e-02,\n",
      "           -2.8443e-01, -6.3517e-02],\n",
      "          [-2.4885e-01, -1.5786e-01,  3.6815e-01,  ..., -4.1959e-01,\n",
      "           -7.1626e-02, -2.1648e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2771e-01,  4.6893e-01,  3.5948e-01,  ..., -4.3454e-01,\n",
      "            2.3437e-01, -2.7227e-01],\n",
      "          [ 1.2277e-01, -3.0000e-04, -2.2478e-01,  ...,  7.9247e-02,\n",
      "           -9.3391e-01, -7.9511e-01],\n",
      "          [ 5.8145e-01, -2.1666e-01,  9.1031e-01,  ..., -8.3941e-01,\n",
      "           -9.0515e-01, -7.1743e-01],\n",
      "          ...,\n",
      "          [-5.4743e-01, -3.8624e-01, -1.5074e-01,  ...,  8.7355e-02,\n",
      "           -7.0458e-02, -3.3608e-01],\n",
      "          [-3.1444e-01, -7.5618e-01,  4.5364e-02,  ..., -2.2603e-01,\n",
      "           -1.7874e-01,  4.1399e-01],\n",
      "          [-9.1563e-02, -1.7720e-01, -2.9868e-01,  ...,  9.2308e-02,\n",
      "            6.1220e-01,  8.1483e-01]],\n",
      "\n",
      "         [[-2.7326e-01, -3.9030e-01,  7.2122e-01,  ..., -4.8389e-01,\n",
      "           -8.1618e-02,  6.8473e-01],\n",
      "          [-5.6936e-01,  9.6630e-01, -3.2124e-01,  ...,  7.2512e-02,\n",
      "            4.0755e-01, -8.3224e-01],\n",
      "          [ 8.0516e-02, -2.3082e-01, -2.7572e-01,  ..., -1.5584e-01,\n",
      "           -6.0014e-01, -3.0779e-01],\n",
      "          ...,\n",
      "          [ 5.2906e-01, -3.0849e-01, -8.3631e-02,  ..., -4.3655e-01,\n",
      "            9.9170e-01,  3.4435e-02],\n",
      "          [-2.9447e-01,  3.8248e-01,  7.3701e-02,  ...,  1.8488e-01,\n",
      "           -1.0310e+00, -2.9073e-01],\n",
      "          [ 3.1156e-01, -3.5395e-01,  3.1898e-01,  ..., -6.9616e-01,\n",
      "            1.8405e-01,  4.7312e-01]],\n",
      "\n",
      "         [[ 4.9482e-02, -1.0364e-01,  1.7018e+00,  ...,  3.3032e-01,\n",
      "           -6.9998e-01,  1.0408e+00],\n",
      "          [-2.5276e-01,  8.1770e-01,  7.2690e-01,  ...,  3.2036e-02,\n",
      "            3.1479e-01, -4.9380e-01],\n",
      "          [-4.7441e-01,  4.3041e-01, -9.8878e-02,  ..., -9.8486e-01,\n",
      "            4.7848e-01,  4.1801e-01],\n",
      "          ...,\n",
      "          [ 3.4931e-01, -5.3400e-02,  9.8480e-01,  ..., -1.8322e-01,\n",
      "            1.2033e+00, -3.0400e-01],\n",
      "          [-1.8338e-01,  7.7590e-01,  4.1222e-01,  ...,  3.4702e-01,\n",
      "           -1.2195e-01,  1.6254e-02],\n",
      "          [ 2.8749e-01,  6.4867e-02,  5.2506e-01,  ...,  3.3474e-01,\n",
      "            1.0391e+00, -2.3278e-02]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n = nn.Conv1d(60, 128, kernel_size=3)\n",
    "\n",
    "input = torch.randn(32, 60, 768) # embed de tape con batch size 32 [batch_size, dimx, dimy]\n",
    "output = n(input)\n",
    "print(output)\n",
    "\n",
    "\n",
    "n1 = nn.Conv2d(1, 64, kernel_size=3)\n",
    "n2 = nn.MaxPool2d(2, 2)  \n",
    "n3 = conv2 = nn.Conv2d(6, 16, 5) \n",
    "n4 = nn.Linear(16*5*5, 120) \n",
    "n5 = nn.Linear(120, 84)\n",
    "n6 = nn.Linear(84, 10)\n",
    "n7 = nn.Softmax()    \n",
    "\n",
    "input = torch.randn(32, 60, 768)\n",
    "input = input.view(32, 1, 60, 768)\n",
    "\n",
    "#input = torch.randn(32, 1, 60, 768)\n",
    "output = n(input)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706e3f9463f4659aea008cbeb97149de47ce197374eec6ebe341c89d29bb6fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
