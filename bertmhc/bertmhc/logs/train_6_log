
# train 6 una BERTMHC_RNN2, esta RNN usa la salida de TAPE (no el average) y se uso 10 epochs
__main__ - Training on 107424 samples, eval on 13428
tape.models.modeling_utils - loading configuration file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-config.json from cache at /home/vicente/.cache/torch/protein_models/fbb05edff0ffa844a729a04850272a1f8973bc002526f6615ad113a5f5aacd36.05edb4ed225e1907a3878f9d68b275d79e025b667555aa94a086e27cb5c591e0
tape.models.modeling_utils - Model config {
  "attention_probs_dropout_prob": 0.1,
  "base_model": "transformer",
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "input_size": 768,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 8192,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_size": 768,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 1,
  "vocab_size": 30
}

tape.models.modeling_utils - loading weights file https://s3.amazonaws.com/songlabdata/proteindata/pytorch-models/bert-base-pytorch_model.bin from cache at /home/vicente/.cache/torch/protein_models/2ed84d28db0a61af4cd2dd3f2ccdd3ee45b1533547a8e1213840af895e2fa8d1.8206daaea9be2736b6ccde432df9dc3dbb8c3233b47f07688d6ff38d74258d22
tape.models.modeling_utils - Weights of BERTMHC_RNN2 not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'rnn.weight_ih_l0_reverse', 'rnn.weight_hh_l0_reverse', 'rnn.bias_ih_l0_reverse', 'rnn.bias_hh_l0_reverse', 'rnn.weight_ih_l1', 'rnn.weight_hh_l1', 'rnn.bias_ih_l1', 'rnn.bias_hh_l1', 'rnn.weight_ih_l1_reverse', 'rnn.weight_hh_l1_reverse', 'rnn.bias_ih_l1_reverse', 'rnn.bias_hh_l1_reverse']
tape.models.modeling_utils - Weights from pretrained model not used in BERTMHC_RNN2: ['mlm.bias', 'mlm.transform.dense.weight', 'mlm.transform.dense.bias', 'mlm.transform.LayerNorm.weight', 'mlm.transform.LayerNorm.bias', 'mlm.decoder.weight']
Training epoch 0
100%|███████████████████████████████████████| 3357/3357 [09:15<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.4728813456224401, 'val_auc': 0.8255427405099536, 'val_ap': 0.7321296707240474, 'val_mass_auc': 0.7946103105560995, 'val_loss': 0.00942002349145575, 'train_loss': 0.01906519111951645}
Validation loss (inf --> -0.825543).  Saving model ...
Training epoch 1
100%|███████████████████████████████████████| 3357/3357 [09:12<00:00,  6.07it/s]
root - Sample dict log: {'val_cor': 0.4842890839748655, 'val_auc': 0.831093148348863, 'val_ap': 0.2771889813181928, 'val_mass_auc': 0.21755215639795739, 'val_loss': 0.009367217717868144, 'train_loss': 0.01883868290467084}
Validation loss (-0.825543 --> -0.831093).  Saving model ...
Training epoch 2
100%|███████████████████████████████████████| 3357/3357 [09:13<00:00,  6.07it/s]
root - Sample dict log: {'val_cor': 0.4759368226380276, 'val_auc': 0.8285102223390449, 'val_ap': 0.26892711584293794, 'val_mass_auc': 0.18632472800961358, 'val_loss': 0.009312176135348392, 'train_loss': 0.018780394141568665}
EarlyStopping counter: 1 out of 5
Training epoch 3
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.45193332888261567, 'val_auc': 0.8152950472673379, 'val_ap': 0.2697281354558956, 'val_mass_auc': 0.18921771401384485, 'val_loss': 0.00943883159773528, 'train_loss': 0.018843824120439834}
EarlyStopping counter: 2 out of 5
Training epoch 4
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.4374950250777748, 'val_auc': 0.8048036243625455, 'val_ap': 0.2713346766436656, 'val_mass_auc': 0.1969560507341041, 'val_loss': 0.009473535674187197, 'train_loss': 0.01902574214070041}
EarlyStopping counter: 3 out of 5
Training epoch 5
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.5074817889078076, 'val_auc': 0.8452389379818485, 'val_ap': 0.28499115651266904, 'val_mass_auc': 0.18148543851360038, 'val_loss': 0.009186028697357882, 'train_loss': 0.0185566041130314}
Validation loss (-0.831093 --> -0.845239).  Saving model ...
Training epoch 6
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.5235599203535576, 'val_auc': 0.8546474267227663, 'val_ap': 0.27101893191084103, 'val_mass_auc': 0.1616156385514727, 'val_loss': 0.00911947234794786, 'train_loss': 0.018340593136487473}
Validation loss (-0.845239 --> -0.854647).  Saving model ...
Training epoch 7
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.5353638423890176, 'val_auc': 0.8611117979223099, 'val_ap': 0.28005419075422044, 'val_mass_auc': 0.16153999910019473, 'val_loss': 0.009087315442344068, 'train_loss': 0.018213442047918645}
Validation loss (-0.854647 --> -0.861112).  Saving model ...
Training epoch 8
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.5424666658767341, 'val_auc': 0.8645364215394168, 'val_ap': 0.27267608798972537, 'val_mass_auc': 0.15507853271175254, 'val_loss': 0.009100478740632623, 'train_loss': 0.0181284151721101}
Validation loss (-0.861112 --> -0.864536).  Saving model ...
Training epoch 9
100%|███████████████████████████████████████| 3357/3357 [09:14<00:00,  6.05it/s]
root - Sample dict log: {'val_cor': 0.5485295364376578, 'val_auc': 0.8683407748723135, 'val_ap': 0.2776306037961656, 'val_mass_auc': 0.15531785941222037, 'val_loss': 0.009030359207092385, 'train_loss': 0.018040547305804067}
Validation loss (-0.864536 --> -0.868341).  Saving model ...
